artifacts_base_dir: artifacts

# =============================================================================
# DATA CONFIGURATION
# =============================================================================
data:
  coins: [BTC, ETH, LTC, XRP, BNB, ADA, XLM, TRX, DOGE, DOT]  # Mix of high-volume and diverse crypto sectors
  target_coin: XRP
  interval: 1h
  default_start_date: 2018-05-05  # XRP prices start varying from 2018-05-04
  default_end_date: 2025-10-25
  data_dir: artifacts/step_01_download
  
  # Collection settings
  collection:
    batch_limit: 1000
    max_retries: 3
    retry_delay: 5
    request_timeout: 30
    rate_limit_delay: 0.2
    min_data_ratio: 0.95
    max_unchanged_threshold: 5

# =============================================================================
# TRAIN/VALIDATION SPLIT CONFIGURATION
# =============================================================================
split:
  train_ratio: 0.8
  temporal: true

# =============================================================================
# TOKENIZATION CONFIGURATION
# =============================================================================
tokenization:
  vocab_size: 256
  method: quantile  # Percentiles will be auto-generated at runtime (0.4 to 99.6)

# =============================================================================
# SEQUENCE CONFIGURATION
# =============================================================================
sequences:
  input_length: 48            # 48 hours (2 days) of historical context
  output_length: 1            # Single prediction
  num_channels: 21            # 21 channels: price, volume + 17 technical indicators + 2 time features
  prediction_horizon: 4        # Predict 4 hours ahead (easier signal)

# =============================================================================
# MODEL ARCHITECTURE
# =============================================================================
model:
  vocab_size: 256
  num_classes: 256
  num_coins: 10
  
  # Architecture (moderate size with ordinal loss)
  d_model: 192
  nhead: 6
  num_encoder_layers: 3
  num_decoder_layers: 2
  dim_feedforward: 768
  dropout: 0.2
  
  # Embeddings
  coin_embedding_dim: 16
  positional_encoding: sinusoidal
  max_seq_len: 1024
  
  # V4 Features
  multi_horizon_enabled: false
  btc_attention_enabled: true
  time_features_enabled: false

# =============================================================================
# TRAINING CONFIGURATION
# =============================================================================
training:
  device: cuda
  epochs: 5
  batch_size: 192
  num_workers: 0
  
  # Optimizer
  learning_rate: 0.001
  weight_decay: 0.0001
  max_grad_norm: 1.0
  
  # Regularization
  dropout: 0.2
  label_smoothing: 0.05
  ordinal_sigma: 8.0  # Spread for soft ordinal targets
  gaussian_noise: 0.0
  
  # Learning rate schedule
  scheduler: warmup_cosine
  warmup_epochs: 3
  warmup_start_lr: 0.00001
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 30
    min_delta: 0.0001
  
  # Walk-forward validation
  walk_forward:
    val_split_hours: 4320  # 6 months for validation

# =============================================================================
# INFERENCE CONFIGURATION
# =============================================================================
inference:
  # Token prediction only - no threshold calibration needed
  pass

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
logging:
  level: INFO
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  file: null
