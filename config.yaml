artifacts_base_dir: artifacts

# Data configuration
data:
  coins:
  - BTC
  - ETH
  - BNB
  - XRP
  - SOL
  - DOGE
  - ADA
  - AVAX
  - DOT
  - LTC
  target_coin: XRP  # Coin to predict
  interval: 1h
  default_start_date: 2020-10-01
  default_end_date: 2025-10-01
  data_dir: artifacts/step_01_download
  collection:
    batch_limit: 1000
    max_retries: 3
    retry_delay: 5
    request_timeout: 30
    rate_limit_delay: 0.2
    min_data_ratio: 0.95
    max_unchanged_threshold: 5

# Split configuration
split:
  train_ratio: 0.8  # 80% train, 20% val
  temporal: true  # Temporal split (no shuffling)

# Tokenization configuration - UPDATED to 256 bins
tokenization:
  vocab_size: 256  # 256-bin quantization (0-255)
  method: quantile  # Use quantile-based binning for uniform distribution
  percentiles: [1, 2, 3, ..., 99]  # 255 bin edges for 256 bins

# Sequence configuration - UPDATED to single-step output
sequences:
  input_length: 24  # 24 hours (1 day) of historical context
  output_length: 1  # Single next-hour prediction (autoregressively generated to 8 hours at inference)
  num_channels: 2  # 0=price, 1=volume

# Model configuration (Simple Token Predictor - Transformer Decoder-Only)
# UPDATED to 256 vocab and corresponding architecture
model:
  type: SimpleTokenPredictor
  vocab_size: 256  # 256-bin vocabulary for continuous quantization
  num_classes: 256  # 256-class classification
  num_coins: 10  # Number of coins (BTC, ETH, BNB, XRP, SOL, DOGE, ADA, AVAX, DOT, LTC)
  embedding_dim: 32  # Original: smaller, efficient
  d_model: 128  # Original: embedding_dim * 4 = 32 * 4
  num_heads: 4  # Original: 4 heads works well
  num_layers: 2  # Original: 2 layers (shallower is better)
  feedforward_dim: 256  # Original
  dropout: 0.11071062737382537  # Original
  stochastic_depth: 0.1  # NEW: Randomly skip residual connections (drop_prob per layer)

# Training configuration
training:
  device: cuda
  epochs: 100
  batch_size: 256  # Original: larger batches
  learning_rate: 0.00121801429078558  # Original
  weight_decay: 0.01  # INCREASED: from 0.0021 to 0.01 (5x stronger L2 regularization)
  label_smoothing: 3.2367826838699014e-05  # Original
  max_grad_norm: 0.8625800840715905  # Original
  gaussian_noise: 0.05  # Input Gaussian noise scale for regularization
  num_workers: 0
  early_stopping:
    enabled: true
    patience: 40  # INCREASED: from 20 to 40 (allow more epochs without improvement)
    min_delta: 0.0001
  warmup:
    enabled: true
    epochs: 3  # Original
    start_lr: 0.0000001  # 1e-7
  scheduler:
    enabled: true
    type: ReduceLROnPlateau
    factor: 0.5
    patience: 10
    min_lr: 0.000001  # 1e-6 - minimum learning rate
  log_interval: 10
  save_interval: 10

# Inference configuration
inference:
  num_steps: 8  # Generate 8 steps autoregressively
  sampling: argmax  # or 'sample' for stochastic generation

# Logging configuration
logging:
  level: INFO
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  file: null
