artifacts_base_dir: artifacts

# Data configuration
data:
  coins:
  - BTC
  - ETH
  - BNB
  - XRP
  - SOL
  - DOGE
  - ADA
  - AVAX
  - DOT
  - LTC
  target_coin: XRP  # Coin to predict
  interval: 1h
  default_start_date: 2020-10-01
  default_end_date: 2025-10-01
  data_dir: artifacts/step_01_download
  collection:
    batch_limit: 1000
    max_retries: 3
    retry_delay: 5
    request_timeout: 30
    rate_limit_delay: 0.2
    min_data_ratio: 0.95
    max_unchanged_threshold: 5

# Split configuration
split:
  train_ratio: 0.8  # 80% train, 20% val
  temporal: true  # Temporal split (no shuffling)

# Tokenization configuration - UPDATED to 256 bins
tokenization:
  vocab_size: 256  # 256-bin quantization (0-255)
  method: quantile  # Use quantile-based binning for uniform distribution
  percentiles: [1, 2, 3, ..., 99]  # 255 bin edges for 256 bins

# Sequence configuration - UPDATED to single-step output
sequences:
  input_length: 24  # 24 hours (1 day) of historical context
  output_length: 1  # Single next-hour prediction (autoregressively generated to 8 hours at inference)
  num_channels: 2  # 0=price, 1=volume

# Model configuration (Simple Token Predictor - Transformer Decoder-Only)
# UPDATED to 256 vocab and corresponding architecture
model:
  type: SimpleTokenPredictor
  vocab_size: 256  # 256-bin vocabulary for continuous quantization
  num_classes: 256  # 256-class classification
  num_coins: 10  # Number of coins (BTC, ETH, BNB, XRP, SOL, DOGE, ADA, AVAX, DOT, LTC)
  embedding_dim: 64  # Token embedding dimension (separate for price + volume)
  d_model: 256  # Hidden dimension after channel fusion (embedding_dim * 2 for 2 channels)
  num_heads: 4  # Number of attention heads (must divide d_model)
  num_layers: 4  # Number of transformer decoder layers
  feedforward_dim: 512  # Feedforward dimension (2x d_model)
  dropout: 0.1  # Dropout rate

# Training configuration
training:
  device: cuda
  epochs: 100
  batch_size: 256
  learning_rate: 0.0001  # 1e-4
  weight_decay: 0.00001  # 1e-5
  label_smoothing: 0.0  # Label smoothing for calibration (0.0-0.2 range)
  max_grad_norm: 1.0  # Gradient clipping
  num_workers: 0
  early_stopping:
    enabled: true
    patience: 20
    min_delta: 0.0001
  warmup:
    enabled: true
    epochs: 5
    start_lr: 0.0000001  # 1e-7
  scheduler:
    enabled: true
    type: ReduceLROnPlateau
    factor: 0.5
    patience: 10
    min_lr: 0.000001  # 1e-6 - minimum learning rate
  log_interval: 10
  save_interval: 10

# Inference configuration
inference:
  num_steps: 8  # Generate 8 steps autoregressively
  sampling: argmax  # or 'sample' for stochastic generation

# Logging configuration
logging:
  level: INFO
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  file: null
