artifacts_base_dir: artifacts

# Data configuration
data:
  coins:
  - BTC
  - ETH
  - BNB
  - XRP
  - SOL
  - DOGE
  - ADA
  - AVAX
  - DOT
  - LTC
  target_coin: XRP  # Coin to predict
  interval: 1h
  default_start_date: 2020-10-01
  default_end_date: 2025-10-01
  data_dir: artifacts/step_01_download
  collection:
    batch_limit: 1000
    max_retries: 3
    retry_delay: 5
    request_timeout: 30
    rate_limit_delay: 0.2
    min_data_ratio: 0.95
    max_unchanged_threshold: 5

# Split configuration
split:
  train_ratio: 0.8  # 80% train, 20% val
  temporal: true  # Temporal split (no shuffling)

# Tokenization configuration - UPDATED to 256 bins
tokenization:
  vocab_size: 256  # 256-bin quantization (0-255)
  method: quantile  # Use quantile-based binning for uniform distribution
  percentiles: [1, 2, 3, ..., 99]  # 255 bin edges for 256 bins

# Sequence configuration - UPDATED to single-step output
sequences:
  input_length: 24  # 24 hours (1 day) of historical context
  output_length: 1  # Single next-hour prediction (autoregressively generated to 8 hours at inference)
  num_channels: 2  # 0=price, 1=volume

# Model configuration (Simple Token Predictor - Transformer Decoder-Only)
# UPDATED to 256 vocab and corresponding architecture
model:
  type: SimpleTokenPredictor
  vocab_size: 256  # 256-bin vocabulary for continuous quantization
  num_classes: 256  # 256-class classification
  num_coins: 10  # Number of coins (BTC, ETH, BNB, XRP, SOL, DOGE, ADA, AVAX, DOT, LTC)
  embedding_dim: 256  # TUNED: from 32 -> 256 (MUCH LARGER!)
  d_model: 1024  # TUNED: from 128 -> 1024 (embedding_dim * 4 = 256 * 4)
  num_heads: 1  # TUNED: from 4 -> 1 (single head!)
  num_layers: 4  # TUNED: from 2 -> 4 (more layers)
  feedforward_dim: 512  # TUNED: from 256 -> 512
  dropout: 0.09640219249606652  # TUNED

# Training configuration
training:
  device: cuda
  epochs: 100
  batch_size: 64  # TUNED: from 256 -> 64 (smaller batches!)
  learning_rate: 0.0008524870889324038  # TUNED
  weight_decay: 1.7550039973175347e-05  # TUNED
  label_smoothing: 7.679525702468799e-05  # TUNED
  max_grad_norm: 1.3419654478570886  # TUNED
  num_workers: 0
  early_stopping:
    enabled: true
    patience: 20
    min_delta: 0.0001
  warmup:
    enabled: true
    epochs: 4  # TUNED: from 3 -> 4
    start_lr: 0.0000001  # 1e-7
  scheduler:
    enabled: true
    type: ReduceLROnPlateau
    factor: 0.5
    patience: 10
    min_lr: 0.000001  # 1e-6 - minimum learning rate
  log_interval: 10
  save_interval: 10

# Inference configuration
inference:
  num_steps: 8  # Generate 8 steps autoregressively
  sampling: argmax  # or 'sample' for stochastic generation

# Logging configuration
logging:
  level: INFO
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  file: null
