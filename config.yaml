artifacts_base_dir: artifacts

# =============================================================================
# DATA CONFIGURATION
# =============================================================================
data:
  coins: [BTC, ETH, LTC, XRP, BNB, ADA, XLM, TRX, DOGE, DOT]  # Mix of high-volume and diverse crypto sectors
  target_coin: XRP
  interval: 1h
  default_start_date: 2018-05-05  # XRP prices start varying from 2018-05-04
  default_end_date: 2025-10-25
  data_dir: artifacts/step_01_download
  
  # Collection settings
  collection:
    batch_limit: 1000
    max_retries: 3
    retry_delay: 5
    request_timeout: 30
    rate_limit_delay: 0.2
    min_data_ratio: 0.95
    max_unchanged_threshold: 5

# =============================================================================
# TRAIN/VALIDATION SPLIT CONFIGURATION
# =============================================================================
split:
  train_ratio: 0.8
  temporal: true

# =============================================================================
# TOKENIZATION CONFIGURATION
# =============================================================================
tokenization:
  vocab_size: 256
  method: quantile  # Percentiles will be auto-generated at runtime (0.4 to 99.6)

# =============================================================================
# SEQUENCE CONFIGURATION
# =============================================================================
sequences:
  input_length: 48            # 48 hours (2 days) of historical context
  output_length: 1            # Single prediction
  num_channels: 9              # 9 channels: price, volume, RSI, MACD, BB position, EMA-9, EMA-21, EMA-50, EMA-ratio
  prediction_horizon: 1     # Predict 1 hour ahead

# =============================================================================
# MODEL ARCHITECTURE
# =============================================================================
model:
  # Model configuration
  vocab_size: 256
  num_classes: 256            # 256-class token prediction
  num_coins: 10
  binary_classification: false
  
  # Architecture (tuned via Optuna) - increased for 10 coins
  d_model: 256
  nhead: 8
  num_encoder_layers: 3
  num_decoder_layers: 2
  dim_feedforward: 1024
  dropout: 0.25
  
  # Embeddings and encoding
  coin_embedding_dim: 16
  positional_encoding: sinusoidal
  max_seq_len: 1024
  
  # V4 Features (simplified for 10-coin training)
  multi_horizon_enabled: false
  btc_attention_enabled: true
  time_features_enabled: false  # Disable time features to reduce complexity

  # Alternative: Use V1 model (simpler, more stable)
  # type: CryptoTransformerV1

  # For testing V1 vs V4:
  # V1: Simpler, more stable, good baseline
  # V4: Complex but potentially better with more data

# =============================================================================
# TRAINING CONFIGURATION
# =============================================================================
training:
  device: cuda
  epochs: 50
  batch_size: 128
  num_workers: 0
  
  # Optimizer (optimized for better validation performance)
  learning_rate: 0.0015  # Slightly higher for faster convergence
  weight_decay: 0.0001   # Reduced to prevent over-regularization
  max_grad_norm: 1.0     # Standard gradient clipping
  
  # Regularization (balanced for 9-channel model)
  dropout: 0.2           # Reduced from 0.25 to prevent underfitting
  label_smoothing: 0.01  # Slight smoothing for better generalization
  gaussian_noise: 0.05   # Reduced noise to prevent information loss
  
  # Learning rate schedule
  scheduler: warmup_cosine
  warmup_epochs: 3
  warmup_start_lr: 0.00001
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 30
    min_delta: 0.0001
  
  # Loss function
  loss_type: distance_weighted  # 'cross_entropy', 'soft_ordinal', 'distance_weighted', or 'ordinal_regression'
  ordinal_sigma: 5.0  # For soft_ordinal: controls spread (1=sharp, 10=wide)
  distance_alpha: 0.05  # For distance_weighted: penalty factor (0.05 achieved best accuracy 0.30%)
  use_class_weights: true

# =============================================================================
# INFERENCE CONFIGURATION
# =============================================================================
inference:
  # 3-class calibration for BUY and SELL signals
  target_signal_rate: 0.02  # 2% target rate for BUY and SELL signals     # Target 5% signal rate for both BUY and SELL
  calibration:
    mode: precision_at_most_rate  # choose top-k <= rate to maximize precision
    min_signals: 20               # avoid extremely tiny k that overfits noise

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
logging:
  level: INFO
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  file: null
