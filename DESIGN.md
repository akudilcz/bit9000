# Simple Multi-Coin Token Predictor - Technical Specification

## Overview

**Objective**: Train a lightweight transformer decoder to predict the next hour's XRP price movement using 24 hours of multi-coin token sequences. Generate 8-hour forecasts autoregressively at inference time.

**Input**: `(24, 10, 2)` - 24 hours √ó 10 coins √ó 2 channels (price + volume)  
**Output**: `(1,)` - Next hour XRP price direction token  
**Vocabulary**: 256 bins `{0-255}` for continuous price quantization  
**Architecture**: Transformer decoder-only with causal masking, autoregressive generation

---

## Data Flow Summary

```
Step 0: Reset
  ‚îî‚îÄ> Clean artifacts directory

Step 1: Download
  ‚îî‚îÄ> OHLCV data (2020-2025, 10 coins, hourly)
      ‚îú‚îÄ raw_data.parquet [T √ó (10 coins √ó 5 OHLCV)]
      ‚îî‚îÄ Visualizations: price trends, data quality

Step 2: Clean
  ‚îî‚îÄ> Fill gaps, remove outliers
      ‚îú‚îÄ clean_data.parquet [T √ó (10 coins √ó 5 OHLCV)]
      ‚îî‚îÄ Quality metrics: % filled, % outliers

Step 3: Split
  ‚îî‚îÄ> Temporal split (80% train, 20% val)
      ‚îú‚îÄ train_clean.parquet [T_train √ó (10 coins √ó 5)]
      ‚îî‚îÄ val_clean.parquet [T_val √ó (10 coins √ó 5)]

Step 4: Tokenize
  ‚îî‚îÄ> Convert price/volume to 256-bin tokens
      ‚îú‚îÄ Compute log returns (price) and log changes (volume)
      ‚îú‚îÄ Fit bin edges (0-255) based on percentiles on train data
      ‚îú‚îÄ Apply bins to train & val
      ‚îú‚îÄ train_tokens.parquet [T_train √ó 20 cols: COIN_price, COIN_volume]
      ‚îú‚îÄ val_tokens.parquet [T_val √ó 20 cols]
      ‚îî‚îÄ fitted_thresholds.json {coin: {price: [bin_edges], volume: [bin_edges]}}

Step 5: Sequences
  ‚îî‚îÄ> Create rolling windows (24h input ‚Üí 1h target)
      ‚îú‚îÄ train_X.pt [(N_train, 24, 10, 2)] - all coins, 2 channels
      ‚îú‚îÄ train_y.pt [(N_train,)] - next hour XRP price token only
      ‚îú‚îÄ val_X.pt [(N_val, 24, 10, 2)]
      ‚îî‚îÄ val_y.pt [(N_val,)]

Step 6: Train
  ‚îî‚îÄ> Transformer decoder with teacher forcing
      ‚îú‚îÄ Embed price & volume tokens separately (256 vocab)
      ‚îú‚îÄ Fuse channels ‚Üí d_model=256
      ‚îú‚îÄ Aggregate coins (mean pooling)
      ‚îú‚îÄ Apply positional encoding
      ‚îú‚îÄ Transformer decoder (4 layers, causal mask)
      ‚îú‚îÄ Output head ‚Üí 256 classes (next 1 hour)
      ‚îú‚îÄ model.pt (best checkpoint)
      ‚îî‚îÄ history.json (loss/accuracy curves)

Step 7: Evaluate
  ‚îî‚îÄ> Autoregressive generation on validation set
      ‚îú‚îÄ Generate 8 steps ahead autoregressively
      ‚îú‚îÄ Per-hour accuracy (hours 1-8)
      ‚îú‚îÄ Sequence accuracy (all 8 correct)
      ‚îú‚îÄ Baseline comparison (persistence, random)
      ‚îî‚îÄ eval_results.json + confusion matrices

Step 8: Inference
  ‚îî‚îÄ> Real-time prediction
      ‚îú‚îÄ Fetch last 24h of OHLCV
      ‚îú‚îÄ Tokenize with fitted bin edges
      ‚îú‚îÄ Autoregressive generation: loop 8 times predicting 1 step at a time
      ‚îú‚îÄ Each step: append predicted token to input, shift window, predict next
      ‚îî‚îÄ predictions.json {hour: 1-8, token, probabilities}
```

**Key Transformations**:
- OHLCV ‚Üí Tokens: `log(price[t]/price[t-1])` ‚Üí quantize to 256 bins ‚Üí `{0-255}`
- Tokens ‚Üí Sequences: sliding window (stride=1) ‚Üí `(24h, 10 coins, 2 ch)` inputs, 1 target
- Training: Teacher forcing with ground truth next-hour target
- Inference: Autoregressive generation - predict 1 step, append to sequence, predict next

---

## Design Principles

1. **Single-step prediction**: Model predicts only the next hour; simplicity and stability
2. **Autoregressive generation**: 8-hour forecasts generated by iterative 1-step predictions
3. **Continuous quantization**: 256 bins provide fine-grained price movement representation
4. **Uniform binning**: Bins fit on training data percentiles for balanced distribution
5. **Multi-coin context**: BTC/ETH patterns inform XRP predictions
6. **No data leakage**: Fit bin edges on training data only
7. **Decoder-only**: Causal masking prevents future information leakage

---

## Pipeline Steps

### Step 0: Reset

**Purpose**: Clear all previous pipeline artifacts and start fresh

**Process**:
1. Remove all contents from `artifacts/` directory
2. Initialize empty directory structure for each pipeline step
3. Create reset metadata artifact

**Artifacts**:
- `reset_artifact.json`: Metadata about reset operation (timestamp, path cleared)

---

### Step 1: Download

**Purpose**: Fetch historical OHLCV (Open, High, Low, Close, Volume) data for 10 cryptocurrencies

**Input**: None (external data source: CoinGecko, Binance, or similar API)

**Output**: Raw OHLCV data in parquet format

**Process**:
1. Query data API for 10 coins: BTC, ETH, BNB, XRP, SOL, DOGE, ADA, AVAX, DOT, LTC
2. Time period: 2020-01-01 to 2025-12-31 (6 years of hourly data)
3. For each coin, fetch: Open, High, Low, Close, Volume
4. Handle missing data gracefully (record gaps)
5. Validate data quality: check for NaNs, zero volumes, price ranges
6. Save as parquet for efficient storage and loading

**Artifacts**:
- `raw_data.parquet`: Shape `[T, 50]` where T is number of hourly timestamps
  - Columns: BTC_open, BTC_high, BTC_low, BTC_close, BTC_volume, ETH_open, ... (5 cols √ó 10 coins)
- `download_artifact.json`: Metadata (coins, date range, rows downloaded, missing data %)
- Visualizations: Price trends over time for each coin, data completeness heatmap

---

### Step 2: Clean

**Purpose**: Fill gaps, handle outliers, and ensure data quality

**Input**: Raw OHLCV data from Step 1

**Output**: Cleaned OHLCV data ready for analysis

**Process**:
1. **Fill gaps**: Forward fill for short gaps (<24 hours), remove coins/periods with long gaps
2. **Outlier detection**: Flag extreme price movements (e.g., >50% in 1 hour) - investigate or interpolate
3. **Volume validation**: Remove zero-volume periods, flag suspicious patterns
4. **Price validation**: Ensure High ‚â• Close ‚â• Low ‚â• 0
5. **Alignment**: Ensure all coins have same timestamps (reindex if needed)
6. **Normalization**: All prices in USD, volumes in base currency units

**Artifacts**:
- `clean_data.parquet`: Same shape as raw data, but cleaned and aligned
  - Shape: `[T', 50]` where T' ‚â§ T (fewer rows if long gaps removed)
- `clean_artifact.json`: Metadata
  - Gaps filled: % and number
  - Outliers detected/fixed: count per coin
  - Time range after cleaning
  - Quality score: 0-100

---

### Step 3: Split

**Purpose**: Temporal train/validation split to prevent data leakage

**Input**: Clean OHLCV data from Step 2

**Output**: Train and validation sets separated by time

**Process**:
1. Calculate split point: 80% of timestamps for training, 20% for validation
2. Temporal split (NOT random): maintain temporal order
   - Example: Train = 2020-2024, Val = 2024-2025
3. Split each of the 10 coins independently (each coin gets same split point)
4. Save as separate parquet files

**Rationale for temporal split**:
- Prevents **data leakage**: Model can't see future data during training
- Reflects real-world deployment: model makes predictions on future unseen data
- Avoids look-ahead bias: validation only contains data after training data

**Artifacts**:
- `train_clean.parquet`: Training set
  - Shape: `[T_train, 50]` where T_train ‚âà 0.8 √ó T'
- `val_clean.parquet`: Validation set
  - Shape: `[T_val, 50]` where T_val ‚âà 0.2 √ó T'
- `split_artifact.json`: Metadata
  - Train rows: T_train, date range
  - Val rows: T_val, date range
  - Split ratio: 0.8 / 0.2
  - No temporal overlap confirmed

### Step 4: Tokenization

**Input**: Clean OHLCV data  
**Output**: Token DataFrames with 2 channels per coin

**Process**:
1. **Fit Phase** (training data only):
   - Compute log returns: `r_price = log(close[t] / close[t-1])`
   - Compute log changes: `r_volume = log(volume[t] / volume[t-1])`
   - Calculate 256 bin edges (0-255) using quantile-based binning per coin per channel
   - Save bin edges: `{coin: {price: [edge_0, edge_1, ..., edge_255], volume: [...]}`

2. **Transform Phase** (train + val):
   - Apply fitted bins using `np.digitize()`:
     - Token = bin_index based on which quantile interval the return falls into
     - Range: 0-255 for each price/volume return

**Artifacts**:
- `train_tokens.parquet`: columns like `BTC_price`, `BTC_volume`, etc. (values 0-255)
- `val_tokens.parquet`
- `fitted_thresholds.json`
- Visualizations: token distribution, bin edge heatmap

### Step 5: Sequence Creation

**Input**: Tokenized DataFrames  
**Output**: PyTorch tensors

**Process**:
- Create rolling windows (stride=1):
  - `X`: 24 consecutive hours √ó all coins √ó 2 channels
  - `y`: next single hour of XRP price token only
- Drop incomplete windows

**Tensor Shapes**:
- `train_X.pt`: `(N_train, 24, 10, 2)` dtype=long
- `train_y.pt`: `(N_train,)` dtype=long (single next-hour token)
- `val_X.pt`: `(N_val, 24, 10, 2)` dtype=long
- `val_y.pt`: `(N_val,)` dtype=long (single next-hour token)

### Step 6: Model Training

**Architecture**: Transformer Decoder-Only

**Components**:
1. **Token Embeddings**: Separate embeddings for price (256√ó64) and volume (256√ó64)
2. **Channel Fusion**: Concatenate and project to `d_model=256`
3. **Coin Aggregation**: Mean pooling across coins per timestep
4. **Positional Encoding**: Sinusoidal (max_len = 24)
5. **Transformer Decoder**: 4 layers, 4 heads, causal masking
6. **Output Head**: Linear projection to 256 classes (predicts next hour only)

**Training**:
- **Loss**: Cross-entropy with teacher forcing
- **Optimizer**: AdamW (lr=1e-4, weight_decay=1e-5)
- **Batch size**: 256
- **Gradient clipping**: max_norm=1.0
- **Scheduler**: ReduceLROnPlateau (factor=0.5, patience=10, min_lr=1e-6)
- **Early stopping**: patience=20 epochs
- **Warmup**: 5 epochs (1e-7 ‚Üí 1e-4)

**Artifacts**:
- `model.pt`: Best checkpoint (by val loss)
- `history.json`: Training metrics
- Visualizations: loss curves, accuracy curves

### Step 7: Evaluation

**Process**:
1. For each validation sample, perform autoregressive generation:
   - Start with 24-hour input window
   - Predict next hour (hour 1)
   - Append prediction to sequence and remove first hour (shift window)
   - Repeat 7 more times to get 8 predictions

**Metrics**:
1. **Per-hour accuracy**: Accuracy at each of 8 prediction steps (hour 1 is most accurate, hour 8 least)
2. **Sequence accuracy**: % where all 8 predictions correct
3. **Confusion matrices**: Per-hour (hours 1, 4, 8)
4. **Baseline comparisons**:
   - Persistence: repeat last token for all 8 hours
   - Random: uniform distribution for each hour

**Target Performance**:
- Hour-1 accuracy > 25% (baseline: ~0.39% random per bin)
- Hour-8 accuracy > 20%
- Sequence accuracy > 0.01%

**Artifacts**:
- `eval_results.json`: All metrics
- Visualizations: accuracy decay, confusion matrices, baseline comparison

### Step 8: Inference

**Process**:
1. Fetch last 24+1 hours of OHLCV data
2. Tokenize using fitted bin edges (2 channels)
3. Create initial tensor: `(1, 24, 10, 2)` (batch_size=1)
4. Loop 8 times:
   - Run model: get logits for next hour (256 classes)
   - Sample/argmax to get predicted token
   - Append prediction to sequence
   - Remove first hour from sequence (keep it 24 hours long)
5. Return 8 predictions with probabilities

**Output Format**:
```json
{
  "timestamp": "2025-10-23T12:00:00Z",
  "coin": "XRP",
  "horizon_hours": 8,
  "predictions": [
    {
      "hour": 1,
      "prediction": 128,
      "confidence": 0.57,
      "probabilities": [0.001, 0.002, ..., 0.57, ...]  # 256 values
    },
    ...
  ]
}
```

---

## Model Specification

### Model Architecture Options

The system supports three model architectures:

#### 1. CryptoTransformerV1 (V1)
**Type**: Decoder-only Transformer (GPT-style)
**Use Case**: Baseline model, fast training, low memory

**Parameters**:
- `vocab_size`: 256
- `embedding_dim`: 64
- `d_model`: 256 (embedding_dim √ó 2 after channel fusion)
- `num_heads`: 4
- `num_layers`: 4
- `feedforward_dim`: 512
- `dropout`: 0.1
- `input_length`: 24
- `num_coins`: 10
- `num_classes`: 256
- `num_channels`: 2

**Architecture**:
1. Token Embeddings: Separate for price and volume (256√ó64 each)
2. Channel Fusion: Concatenate and project to d_model=256
3. Coin Aggregation: Mean pooling across 10 coins
4. Positional Encoding: Sinusoidal
5. Transformer Decoder: 4 layers, 4 heads, causal masking
6. Output Head: Linear(256 ‚Üí 256 classes)

**Model Size**: ~8.5M parameters ‚âà 34 MB (fp32)

**Methods**:
- `forward(x, targets=None)`: Training with teacher forcing, outputs logits (batch, 256)
- `generate(x, steps=8)`: Autoregressive inference, generates steps tokens sequentially

---

#### 2. CryptoTransformerV2 (V2)
**Type**: Enhanced Decoder-only Transformer
**Use Case**: Improved performance, multi-coin attention

**Parameters**:
- `vocab_size`: 256
- `embedding_dim`: 128-256 (configurable)
- `d_model`: 512-1024 (configurable)
- `num_heads`: 4-8
- `num_layers`: 1-6 (tunable)
- `feedforward_dim`: 256-1024
- `dropout`: 0.1-0.3
- `coin_embedding_dim`: 32
- `input_length`: 24-168 hours
- `num_coins`: 10
- `num_classes`: 256

**Architecture Improvements over V1**:
- Coin-specific embeddings (learn each asset's behavior)
- Enhanced channel fusion with layer normalization
- Optional stochastic depth for regularization
- Learned positional encodings (scales better)
- Multi-head coin attention

**Model Size**: ~10M-60M parameters (depending on config)

**Best Tuned Config** (from Optuna):
- embedding_dim: 16, d_model: 64, num_layers: 2, num_heads: 2
- Batch size: 128, LR: 0.00217, dropout: 0.16
- Best val loss: 5.49 (OrdinalRegressionLoss)

---

#### 3. CryptoTransformerV3 (V3) ‚≠ê **RECOMMENDED**
**Type**: Encoder-Decoder Transformer
**Use Case**: State-of-the-art performance, literature-backed design

**Parameters**:
- `vocab_size`: 256
- `num_classes`: 256
- `num_coins`: 10
- `d_model`: 512
- `nhead`: 8
- `num_encoder_layers`: 4
- `num_decoder_layers`: 4
- `dim_feedforward`: 1024
- `dropout`: 0.1
- `coin_embedding_dim`: 32
- `positional_encoding`: 'learned' or 'sinusoidal'
- `max_seq_len`: 256 (supports up to 10+ days of hourly data)

**Multi-Task Heads**:
- **Primary**: 256-class classification (ordinal tokens)
- **Auxiliary 1**: Regression head (expected token index) with Huber loss
- **Auxiliary 2**: Quantile heads (œÑ=0.1, 0.5, 0.9) for uncertainty estimation

**Architecture** (Literature-Backed):
1. **Encoder**: Processes all 10 coins independently
   - Coin embeddings (32-dim per coin)
   - Price & volume embeddings fused per coin
   - Multi-head self-attention across temporal sequence
   - 4 encoder layers with pre-LN (more stable)

2. **Decoder**: Focuses on target coin (XRP)
   - Cross-attention to encoder memory (multi-asset context)
   - Causal self-attention (no future leakage)
   - 4 decoder layers with pre-LN

3. **Multi-Task Heads**:
   - Classification: Linear(d_model ‚Üí 256) for token prediction
   - Regression: MLP(d_model ‚Üí 1) for expected continuous value
   - Quantiles: 3 √ó MLP(d_model ‚Üí 1) for uncertainty bounds

**Model Size**: ~15M parameters ‚âà 60 MB (fp32)

**Advantages over V1/V2**:
- ‚úÖ Separate encoder/decoder roles (better than decoder-only)
- ‚úÖ Cross-attention captures multi-asset correlations
- ‚úÖ Multi-task learning stabilizes training
- ‚úÖ Regression head improves ordinal awareness
- ‚úÖ Quantile heads provide uncertainty estimates
- ‚úÖ Deeper architecture (4+4 layers vs 1-4 in V1/V2)
- ‚úÖ Scales to 7-day context (168 hours)

**Literature Support**:
- Encoder-Decoder: Vaswani et al. (2017) - Attention Is All You Need
- Multi-Task Learning: Caruana (1997) - improves generalization
- Huber Loss: Huber (1964) - robust to outliers in financial data
- Pre-LN: Xiong et al. (2020) - more stable than post-LN

**Configuration Example**:
```yaml
model:
  type: CryptoTransformerV3
  d_model: 512
  nhead: 8
  num_encoder_layers: 4
  num_decoder_layers: 4
  dim_feedforward: 1024
  dropout: 0.1
  coin_embedding_dim: 32
  positional_encoding: learned
  multitask_enabled: true
  enable_regression: true
  enable_quantiles: false
```

---

### Loss Function Options

The system supports four ordinal-aware loss functions:

#### 1. CrossEntropyLoss (Baseline)
**Type**: Standard classification loss
**Formula**: `-log(p_true_class)`

**Characteristics**:
- Treats all misclassifications equally
- Predicting token 56 vs 200 has same penalty as 56 vs 57
- Fast and stable
- No ordinal structure awareness

**Use Case**: Baseline comparison only

**Parameters**: 
- `label_smoothing`: Optional smoothing (default: 0.0)

**Performance**: Not recommended for ordinal tokens

---

#### 2. SmoothOrdinalLoss ‚≠ê **GOOD**
**Type**: Soft-label Gaussian smoothing
**Formula**: KL divergence between softmax(logits) and Gaussian(true_class, œÉ)

**How It Works**:
- Creates soft target distribution centered at true class
- Nearby classes get non-zero probability based on distance
- Example: If true=100, then class 99,101 get ~0.8 prob, 98,102 get ~0.6, etc.
- Uses Gaussian kernel: `exp(-distance¬≤ / (2œÉ¬≤))` or `exp(-distance‚Å¥ / (2œÉ¬≤))` (squared penalty)

**Characteristics**:
- ‚úÖ Rewards nearby predictions (solves original problem)
- ‚úÖ Smooth gradients, stable training
- ‚úÖ Flexible penalty via œÉ parameter
- ‚ö†Ô∏è Slower due to per-sample loop
- ‚ö†Ô∏è Can plateau if œÉ too small

**Parameters**:
- `ordinal_sigma`: Standard deviation (default: 5.0)
  - Smaller œÉ = stricter (only nearest neighbors rewarded)
  - Larger œÉ = more lenient (distant tokens get some reward)
  - Recommended: 3-10

**Configuration**:
```yaml
training:
  loss_type: smooth_ordinal
  ordinal_sigma: 5.0  # Distance ~5 tokens gets ~0.6 probability
```

**Performance**: Val loss ~5.52, stable training

---

#### 3. OrdinalRegressionLoss üèÜ **BEST LOSS**
**Type**: Cumulative probability constraints
**Formula**: Penalizes violations of ordinal structure via cumulative logits

**How It Works**:
- Treats classification as ordinal regression
- For K classes (0-255), maintains cumulative probabilities P(y ‚â§ k)
- Enforces: P(y ‚â§ j) should be low if j < true_class
- Enforces: P(y ‚â§ j) should be high if j ‚â• true_class
- Uses log-sigmoid for numerical stability

**Characteristics**:
- ‚úÖ Best validation loss (5.49) in experiments
- ‚úÖ Theoretically sound for ordinal data
- ‚úÖ Enforces global ordinal constraints
- ‚ö†Ô∏è Can plateau at ~5.2 (strict constraints hard to satisfy)
- ‚ö†Ô∏è More complex than distance-weighted

**Parameters**:
- `ordinal_margin`: Soft margin for constraints (default: 0.5)

**Configuration**:
```yaml
training:
  loss_type: ordinal
  ordinal_margin: 0.5
```

**Performance**: Best val loss 5.49, but plateaus around 5.2

---

#### 4. DistanceWeightedCrossEntropy ‚≠êüèÜ **BEST ACCURACY**
**Type**: Distance-weighted classification
**Formula**: `loss = -log(p_true) √ó (1 + Œ± √ó distance)`

**How It Works**:
- Standard cross-entropy weighted by prediction distance
- If prediction is wrong, penalty scales linearly with distance
- Distance = |predicted_token - true_token|
- Example: Predicting 56 vs 57 ‚Üí penalty √ó 1.05, vs 200 ‚Üí penalty √ó 8.2

**Characteristics**:
- ‚úÖ **Best accuracy (0.303%)** in experiments
- ‚úÖ Simplest ordinal-aware loss
- ‚úÖ Stable convergence (92 epochs without plateauing)
- ‚úÖ Fast computation (no loops)
- ‚úÖ Pragmatic for noisy financial data
- ‚úÖ Balances ordinal awareness with flexibility

**Parameters**:
- `distance_alpha`: Distance penalty factor (default: 0.05)
  - Œ±=0.01: 10% penalty per 10 tokens distance (mild)
  - Œ±=0.05: 50% penalty per 10 tokens distance (moderate) ‚≠ê **RECOMMENDED**
  - Œ±=0.1: 100% penalty per 10 tokens distance (strict)

**Configuration**:
```yaml
training:
  loss_type: distance_weighted
  distance_alpha: 0.05  # 5% extra penalty per token distance
```

**Performance**: Val loss 5.50, accuracy 0.303%, 92 epochs stable training

**Why It's Best**:
- Crypto prices are noisy ‚Üí strict ordinal constraints too rigid
- Rewards nearby predictions (solves core problem)
- Allows model to learn exceptions when data demands it
- Scales well with deeper models and longer sequences

---

### Multi-Task Learning (V3 Only)

CryptoTransformerV3 supports multi-task objectives:

**Combined Loss**:
```
L_total = w_cls √ó L_classification + w_huber √ó L_regression + w_quantile √ó L_quantile
```

**Components**:
1. **Classification Loss**: Any of the 4 losses above
2. **Regression Loss**: Huber loss on expected token index (continuous)
   - Helps model learn ordinal structure via continuous target
   - Robust to outliers (combines L1 + L2)
3. **Quantile Loss**: Pinball loss for œÑ ‚àà {0.1, 0.5, 0.9}
   - Provides uncertainty bounds
   - Optional (disabled by default)

**Default Weights**:
- w_cls: 1.0 (primary task)
- w_huber: 0.3 (auxiliary, helps ordinal learning)
- w_quantile: 0.0 (disabled by default)

**Configuration**:
```yaml
training:
  loss_type: distance_weighted
  distance_alpha: 0.05
  multitask:
    enabled: true
    w_cls: 1.0
    w_huber: 0.3
    w_quantile: 0.0  # Enable with 0.1-0.2 if needed
```

---

### Loss Function Comparison

| Loss | Val Loss | Accuracy | Training Speed | Ordinal Awareness | Stability | Recommendation |
|------|----------|----------|----------------|-------------------|-----------|----------------|
| CrossEntropyLoss | N/A | Low | Fast | ‚ùå None | ‚úÖ High | ‚ùå Not recommended |
| SmoothOrdinalLoss | 5.52 | 0.28% | Slow | ‚úÖ Soft | ‚ö†Ô∏è Can plateau | ‚úÖ Good |
| OrdinalRegressionLoss | **5.49** | 0.28% | Medium | ‚úÖ Strong | ‚ö†Ô∏è Plateaus @5.2 | ‚úÖ Best loss |
| DistanceWeightedCE | 5.50 | **0.30%** | Fast | ‚úÖ Pragmatic | ‚úÖ Excellent | üèÜ **Best overall** |

**Recommendation for V3**:
- **Primary**: DistanceWeightedCrossEntropy with Œ±=0.05
- **Auxiliary**: Enable regression head (w_huber=0.3)
- **Optional**: Add quantile loss (w_quantile=0.1) for uncertainty

---

### Model Selection Guide

| Scenario | Recommended Model | Recommended Loss | Why |
|----------|------------------|------------------|-----|
| **Quick prototype** | CryptoTransformerV1 | DistanceWeightedCE | Fast, simple, proven |
| **Production (best accuracy)** | CryptoTransformerV3 | DistanceWeightedCE + Huber | State-of-the-art, multi-task |
| **Limited GPU memory** | CryptoTransformerV1 | DistanceWeightedCE | Smallest model (~8M params) |
| **Research/experimentation** | CryptoTransformerV3 | Any loss + multitask | Flexible, extensible |
| **7-day context** | CryptoTransformerV3 | DistanceWeightedCE + Huber | Scales to 168 hours |
| **Baseline comparison** | CryptoTransformerV1 | CrossEntropyLoss | Standard benchmark |

---

## Configuration (config.yaml)

```yaml
data:
  coins: [BTC, ETH, BNB, XRP, SOL, DOGE, ADA, AVAX, DOT, LTC]
  target_coin: XRP
  interval: 1h

split:
  train_ratio: 0.8
  temporal: true

tokenization:
  vocab_size: 256
  method: quantile
  percentiles: [1, 2, 3, ..., 99]  # 255 bin edges for 256 bins

sequences:
  input_length: 24
  output_length: 1  # Single next-hour prediction
  num_channels: 2

model:
  type: CryptoTransformerV1
  vocab_size: 256
  num_classes: 256
  num_coins: 10
  embedding_dim: 64
  d_model: 256
  num_heads: 4
  num_layers: 4
  feedforward_dim: 512
  dropout: 0.1

training:
  device: cuda
  epochs: 100
  batch_size: 256
  learning_rate: 0.0001
  weight_decay: 0.00001
  label_smoothing: 0.0
  max_grad_norm: 1.0
  early_stopping:
    patience: 20
  warmup:
    epochs: 5
    start_lr: 0.0000001
  scheduler:
    type: ReduceLROnPlateau
    factor: 0.5
    patience: 10
    min_lr: 0.000001

inference:
  num_steps: 8  # Generate 8 steps autoregressively
  sampling: argmax  # or 'sample' for stochastic generation
```

---

## Artifact Structure

```
artifacts/
‚îú‚îÄ‚îÄ step_04_tokenize/
‚îÇ   ‚îú‚îÄ‚îÄ train_tokens.parquet
‚îÇ   ‚îú‚îÄ‚îÄ val_tokens.parquet
‚îÇ   ‚îú‚îÄ‚îÄ fitted_thresholds.json
‚îÇ   ‚îî‚îÄ‚îÄ tokenize_artifact.json
‚îú‚îÄ‚îÄ step_05_sequences/
‚îÇ   ‚îú‚îÄ‚îÄ train_X.pt  # (N, 24, 10, 2)
‚îÇ   ‚îú‚îÄ‚îÄ train_y.pt  # (N, 8)
‚îÇ   ‚îú‚îÄ‚îÄ val_X.pt
‚îÇ   ‚îú‚îÄ‚îÄ val_y.pt
‚îÇ   ‚îî‚îÄ‚îÄ sequences_artifact.json
‚îú‚îÄ‚îÄ step_06_train/
‚îÇ   ‚îú‚îÄ‚îÄ model.pt
‚îÇ   ‚îú‚îÄ‚îÄ history.json
‚îÇ   ‚îî‚îÄ‚îÄ train_artifact.json
‚îú‚îÄ‚îÄ step_07_evaluate/
‚îÇ   ‚îú‚îÄ‚îÄ eval_results.json
‚îÇ   ‚îî‚îÄ‚îÄ evaluate_artifact.json
‚îî‚îÄ‚îÄ step_08_inference/
    ‚îú‚îÄ‚îÄ predictions_{timestamp}.json
    ‚îî‚îÄ‚îÄ inference_artifact.json
```

---

## Data Schemas

### TokenizeArtifact
```python
{
  "train_path": str,
  "val_path": str,
  "train_shape": tuple,
  "val_shape": tuple,
  "thresholds_path": str,
  "token_distribution": {
    bin_id: {"train": float, "val": float}
    for bin_id in range(256)
  }
}
```

### SequencesArtifact
```python
{
  "train_X_path": str,
  "train_y_path": str,
  "val_X_path": str,
  "val_y_path": str,
  "train_num_samples": int,
  "val_num_samples": int,
  "input_length": 24,
  "output_length": 1,  # Single next-hour prediction
  "num_coins": 10,
  "num_channels": 2,
  "target_coin": "XRP",
  "vocab_size": 256
}
```

### TrainedModelArtifact
```python
{
  "model_path": str,
  "history_path": str,
  "best_val_loss": float,
  "best_val_acc": float,
  "total_epochs": int
}
```

### EvalReportArtifact
```python
{
  "per_hour_accuracy": [float] * 8,
  "sequence_accuracy": float,
  "baseline_results": {
    "random": float,
    "persistence": float
  }
}
```

---

## Key Design Decisions

| Decision | Rationale |
|----------|-----------|
| 24-hour input | Captures daily cycles, trains faster than 48h |
| 2 channels | Price + volume captures strength & direction |
| 256-bin vocab | Fine-grained continuous quantization, interpretable |
| Quantile binning | Auto-balanced distribution, coin-adaptive |
| Fit on train only | Prevents data leakage |
| Decoder-only | Causal masking prevents future leakage |
| Teacher forcing | Stable training, fast convergence |
| 8-hour horizon | Practical for trading, measurable |

---

## Success Criteria

### Pipeline Validation
- ‚úÖ Tokenization produces uniform distribution on train (each bin ~0.39%)
- ‚úÖ No data leakage (val distribution may differ)
- ‚úÖ All tensor shapes correct
- ‚úÖ Token values in range [0, 255]

### Model Performance
- üéØ Hour-1 accuracy > 25% (baseline: ~0.39% random per bin)
- üéØ Hour-8 accuracy > 20%
- üéØ Sequence accuracy > 0.01%
- üéØ Beats all baselines significantly

### Deployment
- ‚úÖ Inference < 100ms
- ‚úÖ Model size < 500MB
- ‚úÖ Reproducible results

---

## Known Limitations

1. **Accuracy challenge**: 256 classes is significantly harder than 3 classes
2. **Accuracy degrades over horizon**: 8-hour predictions inherently uncertain
3. **No uncertainty calibration**: Softmax probabilities ‚â† true confidence
4. **Macro event blindness**: Model can't handle news/crashes outside training distribution
5. **Single-coin output**: Only predicts XRP
6. **Fixed 8-hour horizon**: Not adaptive to user preferences

---

## Computational Requirements

- **Training**: 1 GPU (RTX 3080+), ~3-6 hours for 100 epochs (larger vocab than 3-class), 3-5 GB GPU memory
- **Inference**: CPU sufficient, <50ms per prediction
- **Storage**: <200 MB total (data + model)

---

## Implementation Notes

1. **Anti-leakage**: Bin edges must be fit on train data only, then applied to val/inference
2. **Single-step output**: Model predicts only 1 token (next hour), not 8
3. **Teacher forcing**: During training, use ground truth next-hour token as target
4. **Autoregressive generation**: During eval/inference, loop 8 times:
   - Forward pass through decoder
   - Get logits for all 256 classes
   - Select token (argmax or sample)
   - Append predicted token to sequence
   - Remove first (oldest) hour to maintain 24-hour window
   - Repeat
5. **Causal masking**: Position i can only attend to positions ‚â§ i (prevents future leakage)
6. **Channel handling**: Separate embeddings for price/volume, then fuse
7. **Target extraction**: Only XRP price channel used for targets, not volume
8. **Temporal separation**: Input = hours [0-23], Target = hour [24] (no overlap)

---

## CLI Interface

```bash
# Full pipeline
python main.py pipeline run-all

# Individual steps
python main.py pipeline tokenize
python main.py pipeline sequences
python main.py pipeline train
python main.py pipeline evaluate
python main.py pipeline inference
```

---

## Testing Requirements

1. **Unit tests**: Each pipeline block, model forward/generate
2. **Integration tests**: End-to-end pipeline execution
3. **Shape validation**: All tensor dimensions correct
4. **Leakage detection**: Val thresholds ‚â† train thresholds
5. **Reproducibility**: Same seed ‚Üí same results
