# Simple Multi-Coin Token Predictor - Technical Specification

## Overview

**Objective**: Train a lightweight transformer decoder to predict the next hour's XRP price movement using 24 hours of multi-coin token sequences. Generate 8-hour forecasts autoregressively at inference time.

**Input**: `(24, 10, 2)` - 24 hours × 10 coins × 2 channels (price + volume)  
**Output**: `(1,)` - Next hour XRP price direction token  
**Vocabulary**: 256 bins `{0-255}` for continuous price quantization  
**Architecture**: Transformer decoder-only with causal masking, autoregressive generation

---

## Data Flow Summary

```
Step 0: Reset
  └─> Clean artifacts directory

Step 1: Download
  └─> OHLCV data (2020-2025, 10 coins, hourly)
      ├─ raw_data.parquet [T × (10 coins × 5 OHLCV)]
      └─ Visualizations: price trends, data quality

Step 2: Clean
  └─> Fill gaps, remove outliers
      ├─ clean_data.parquet [T × (10 coins × 5 OHLCV)]
      └─ Quality metrics: % filled, % outliers

Step 3: Split
  └─> Temporal split (80% train, 20% val)
      ├─ train_clean.parquet [T_train × (10 coins × 5)]
      └─ val_clean.parquet [T_val × (10 coins × 5)]

Step 4: Tokenize
  └─> Convert price/volume to 256-bin tokens
      ├─ Compute log returns (price) and log changes (volume)
      ├─ Fit bin edges (0-255) based on percentiles on train data
      ├─ Apply bins to train & val
      ├─ train_tokens.parquet [T_train × 20 cols: COIN_price, COIN_volume]
      ├─ val_tokens.parquet [T_val × 20 cols]
      └─ fitted_thresholds.json {coin: {price: [bin_edges], volume: [bin_edges]}}

Step 5: Sequences
  └─> Create rolling windows (24h input → 1h target)
      ├─ train_X.pt [(N_train, 24, 10, 2)] - all coins, 2 channels
      ├─ train_y.pt [(N_train,)] - next hour XRP price token only
      ├─ val_X.pt [(N_val, 24, 10, 2)]
      └─ val_y.pt [(N_val,)]

Step 6: Train
  └─> Transformer decoder with teacher forcing
      ├─ Embed price & volume tokens separately (256 vocab)
      ├─ Fuse channels → d_model=256
      ├─ Aggregate coins (mean pooling)
      ├─ Apply positional encoding
      ├─ Transformer decoder (4 layers, causal mask)
      ├─ Output head → 256 classes (next 1 hour)
      ├─ model.pt (best checkpoint)
      └─ history.json (loss/accuracy curves)

Step 7: Evaluate
  └─> Autoregressive generation on validation set
      ├─ Generate 8 steps ahead autoregressively
      ├─ Per-hour accuracy (hours 1-8)
      ├─ Sequence accuracy (all 8 correct)
      ├─ Baseline comparison (persistence, random)
      └─ eval_results.json + confusion matrices

Step 8: Inference
  └─> Real-time prediction
      ├─ Fetch last 24h of OHLCV
      ├─ Tokenize with fitted bin edges
      ├─ Autoregressive generation: loop 8 times predicting 1 step at a time
      ├─ Each step: append predicted token to input, shift window, predict next
      └─ predictions.json {hour: 1-8, token, probabilities}
```

**Key Transformations**:
- OHLCV → Tokens: `log(price[t]/price[t-1])` → quantize to 256 bins → `{0-255}`
- Tokens → Sequences: sliding window (stride=1) → `(24h, 10 coins, 2 ch)` inputs, 1 target
- Training: Teacher forcing with ground truth next-hour target
- Inference: Autoregressive generation - predict 1 step, append to sequence, predict next

---

## Design Principles

1. **Single-step prediction**: Model predicts only the next hour; simplicity and stability
2. **Autoregressive generation**: 8-hour forecasts generated by iterative 1-step predictions
3. **Continuous quantization**: 256 bins provide fine-grained price movement representation
4. **Uniform binning**: Bins fit on training data percentiles for balanced distribution
5. **Multi-coin context**: BTC/ETH patterns inform XRP predictions
6. **No data leakage**: Fit bin edges on training data only
7. **Decoder-only**: Causal masking prevents future information leakage

---

## Pipeline Steps

### Step 0: Reset

**Purpose**: Clear all previous pipeline artifacts and start fresh

**Process**:
1. Remove all contents from `artifacts/` directory
2. Initialize empty directory structure for each pipeline step
3. Create reset metadata artifact

**Artifacts**:
- `reset_artifact.json`: Metadata about reset operation (timestamp, path cleared)

---

### Step 1: Download

**Purpose**: Fetch historical OHLCV (Open, High, Low, Close, Volume) data for 10 cryptocurrencies

**Input**: None (external data source: CoinGecko, Binance, or similar API)

**Output**: Raw OHLCV data in parquet format

**Process**:
1. Query data API for 10 coins: BTC, ETH, BNB, XRP, SOL, DOGE, ADA, AVAX, DOT, LTC
2. Time period: 2020-01-01 to 2025-12-31 (6 years of hourly data)
3. For each coin, fetch: Open, High, Low, Close, Volume
4. Handle missing data gracefully (record gaps)
5. Validate data quality: check for NaNs, zero volumes, price ranges
6. Save as parquet for efficient storage and loading

**Artifacts**:
- `raw_data.parquet`: Shape `[T, 50]` where T is number of hourly timestamps
  - Columns: BTC_open, BTC_high, BTC_low, BTC_close, BTC_volume, ETH_open, ... (5 cols × 10 coins)
- `download_artifact.json`: Metadata (coins, date range, rows downloaded, missing data %)
- Visualizations: Price trends over time for each coin, data completeness heatmap

---

### Step 2: Clean

**Purpose**: Fill gaps, handle outliers, and ensure data quality

**Input**: Raw OHLCV data from Step 1

**Output**: Cleaned OHLCV data ready for analysis

**Process**:
1. **Fill gaps**: Forward fill for short gaps (<24 hours), remove coins/periods with long gaps
2. **Outlier detection**: Flag extreme price movements (e.g., >50% in 1 hour) - investigate or interpolate
3. **Volume validation**: Remove zero-volume periods, flag suspicious patterns
4. **Price validation**: Ensure High ≥ Close ≥ Low ≥ 0
5. **Alignment**: Ensure all coins have same timestamps (reindex if needed)
6. **Normalization**: All prices in USD, volumes in base currency units

**Artifacts**:
- `clean_data.parquet`: Same shape as raw data, but cleaned and aligned
  - Shape: `[T', 50]` where T' ≤ T (fewer rows if long gaps removed)
- `clean_artifact.json`: Metadata
  - Gaps filled: % and number
  - Outliers detected/fixed: count per coin
  - Time range after cleaning
  - Quality score: 0-100

---

### Step 3: Split

**Purpose**: Temporal train/validation split to prevent data leakage

**Input**: Clean OHLCV data from Step 2

**Output**: Train and validation sets separated by time

**Process**:
1. Calculate split point: 80% of timestamps for training, 20% for validation
2. Temporal split (NOT random): maintain temporal order
   - Example: Train = 2020-2024, Val = 2024-2025
3. Split each of the 10 coins independently (each coin gets same split point)
4. Save as separate parquet files

**Rationale for temporal split**:
- Prevents **data leakage**: Model can't see future data during training
- Reflects real-world deployment: model makes predictions on future unseen data
- Avoids look-ahead bias: validation only contains data after training data

**Artifacts**:
- `train_clean.parquet`: Training set
  - Shape: `[T_train, 50]` where T_train ≈ 0.8 × T'
- `val_clean.parquet`: Validation set
  - Shape: `[T_val, 50]` where T_val ≈ 0.2 × T'
- `split_artifact.json`: Metadata
  - Train rows: T_train, date range
  - Val rows: T_val, date range
  - Split ratio: 0.8 / 0.2
  - No temporal overlap confirmed

### Step 4: Tokenization

**Input**: Clean OHLCV data  
**Output**: Token DataFrames with 2 channels per coin

**Process**:
1. **Fit Phase** (training data only):
   - Compute log returns: `r_price = log(close[t] / close[t-1])`
   - Compute log changes: `r_volume = log(volume[t] / volume[t-1])`
   - Calculate 256 bin edges (0-255) using quantile-based binning per coin per channel
   - Save bin edges: `{coin: {price: [edge_0, edge_1, ..., edge_255], volume: [...]}`

2. **Transform Phase** (train + val):
   - Apply fitted bins using `np.digitize()`:
     - Token = bin_index based on which quantile interval the return falls into
     - Range: 0-255 for each price/volume return

**Artifacts**:
- `train_tokens.parquet`: columns like `BTC_price`, `BTC_volume`, etc. (values 0-255)
- `val_tokens.parquet`
- `fitted_thresholds.json`
- Visualizations: token distribution, bin edge heatmap

### Step 5: Sequence Creation

**Input**: Tokenized DataFrames  
**Output**: PyTorch tensors

**Process**:
- Create rolling windows (stride=1):
  - `X`: 24 consecutive hours × all coins × 2 channels
  - `y`: next single hour of XRP price token only
- Drop incomplete windows

**Tensor Shapes**:
- `train_X.pt`: `(N_train, 24, 10, 2)` dtype=long
- `train_y.pt`: `(N_train,)` dtype=long (single next-hour token)
- `val_X.pt`: `(N_val, 24, 10, 2)` dtype=long
- `val_y.pt`: `(N_val,)` dtype=long (single next-hour token)

### Step 6: Model Training

**Architecture**: Transformer Decoder-Only

**Components**:
1. **Token Embeddings**: Separate embeddings for price (256×64) and volume (256×64)
2. **Channel Fusion**: Concatenate and project to `d_model=256`
3. **Coin Aggregation**: Mean pooling across coins per timestep
4. **Positional Encoding**: Sinusoidal (max_len = 24)
5. **Transformer Decoder**: 4 layers, 4 heads, causal masking
6. **Output Head**: Linear projection to 256 classes (predicts next hour only)

**Training**:
- **Loss**: Cross-entropy with teacher forcing
- **Optimizer**: AdamW (lr=1e-4, weight_decay=1e-5)
- **Batch size**: 256
- **Gradient clipping**: max_norm=1.0
- **Scheduler**: ReduceLROnPlateau (factor=0.5, patience=10, min_lr=1e-6)
- **Early stopping**: patience=20 epochs
- **Warmup**: 5 epochs (1e-7 → 1e-4)

**Artifacts**:
- `model.pt`: Best checkpoint (by val loss)
- `history.json`: Training metrics
- Visualizations: loss curves, accuracy curves

### Step 7: Evaluation

**Process**:
1. For each validation sample, perform autoregressive generation:
   - Start with 24-hour input window
   - Predict next hour (hour 1)
   - Append prediction to sequence and remove first hour (shift window)
   - Repeat 7 more times to get 8 predictions

**Metrics**:
1. **Per-hour accuracy**: Accuracy at each of 8 prediction steps (hour 1 is most accurate, hour 8 least)
2. **Sequence accuracy**: % where all 8 predictions correct
3. **Confusion matrices**: Per-hour (hours 1, 4, 8)
4. **Baseline comparisons**:
   - Persistence: repeat last token for all 8 hours
   - Random: uniform distribution for each hour

**Target Performance**:
- Hour-1 accuracy > 25% (baseline: ~0.39% random per bin)
- Hour-8 accuracy > 20%
- Sequence accuracy > 0.01%

**Artifacts**:
- `eval_results.json`: All metrics
- Visualizations: accuracy decay, confusion matrices, baseline comparison

### Step 8: Inference

**Process**:
1. Fetch last 24+1 hours of OHLCV data
2. Tokenize using fitted bin edges (2 channels)
3. Create initial tensor: `(1, 24, 10, 2)` (batch_size=1)
4. Loop 8 times:
   - Run model: get logits for next hour (256 classes)
   - Sample/argmax to get predicted token
   - Append prediction to sequence
   - Remove first hour from sequence (keep it 24 hours long)
5. Return 8 predictions with probabilities

**Output Format**:
```json
{
  "timestamp": "2025-10-23T12:00:00Z",
  "coin": "XRP",
  "horizon_hours": 8,
  "predictions": [
    {
      "hour": 1,
      "prediction": 128,
      "confidence": 0.57,
      "probabilities": [0.001, 0.002, ..., 0.57, ...]  # 256 values
    },
    ...
  ]
}
```

---

## Model Specification

### SimpleTokenPredictor

**Parameters**:
- `vocab_size`: 256
- `embedding_dim`: 64
- `d_model`: 256 (embedding_dim × 2 after channel fusion)
- `num_heads`: 4
- `num_layers`: 4
- `feedforward_dim`: 512
- `dropout`: 0.1
- `input_length`: 24
- `num_coins`: 10
- `num_classes`: 256
- `num_channels`: 2

**Methods**:
- `forward(x, targets=None)`: Training with teacher forcing, outputs logits (batch_size, 256)
- `generate(x, steps=8)`: Autoregressive inference, generates `steps` tokens sequentially

**Model Size**: ~8.5M parameters ≈ 34 MB (fp32) - larger due to 256 vocab

---

## Configuration (config.yaml)

```yaml
data:
  coins: [BTC, ETH, BNB, XRP, SOL, DOGE, ADA, AVAX, DOT, LTC]
  target_coin: XRP
  interval: 1h

split:
  train_ratio: 0.8
  temporal: true

tokenization:
  vocab_size: 256
  method: quantile
  percentiles: [1, 2, 3, ..., 99]  # 255 bin edges for 256 bins

sequences:
  input_length: 24
  output_length: 1  # Single next-hour prediction
  num_channels: 2

model:
  type: SimpleTokenPredictor
  vocab_size: 256
  num_classes: 256
  num_coins: 10
  embedding_dim: 64
  d_model: 256
  num_heads: 4
  num_layers: 4
  feedforward_dim: 512
  dropout: 0.1

training:
  device: cuda
  epochs: 100
  batch_size: 256
  learning_rate: 0.0001
  weight_decay: 0.00001
  label_smoothing: 0.0
  max_grad_norm: 1.0
  early_stopping:
    patience: 20
  warmup:
    epochs: 5
    start_lr: 0.0000001
  scheduler:
    type: ReduceLROnPlateau
    factor: 0.5
    patience: 10
    min_lr: 0.000001

inference:
  num_steps: 8  # Generate 8 steps autoregressively
  sampling: argmax  # or 'sample' for stochastic generation
```

---

## Artifact Structure

```
artifacts/
├── step_04_tokenize/
│   ├── train_tokens.parquet
│   ├── val_tokens.parquet
│   ├── fitted_thresholds.json
│   └── tokenize_artifact.json
├── step_05_sequences/
│   ├── train_X.pt  # (N, 24, 10, 2)
│   ├── train_y.pt  # (N, 8)
│   ├── val_X.pt
│   ├── val_y.pt
│   └── sequences_artifact.json
├── step_06_train/
│   ├── model.pt
│   ├── history.json
│   └── train_artifact.json
├── step_07_evaluate/
│   ├── eval_results.json
│   └── evaluate_artifact.json
└── step_08_inference/
    ├── predictions_{timestamp}.json
    └── inference_artifact.json
```

---

## Data Schemas

### TokenizeArtifact
```python
{
  "train_path": str,
  "val_path": str,
  "train_shape": tuple,
  "val_shape": tuple,
  "thresholds_path": str,
  "token_distribution": {
    bin_id: {"train": float, "val": float}
    for bin_id in range(256)
  }
}
```

### SequencesArtifact
```python
{
  "train_X_path": str,
  "train_y_path": str,
  "val_X_path": str,
  "val_y_path": str,
  "train_num_samples": int,
  "val_num_samples": int,
  "input_length": 24,
  "output_length": 1,  # Single next-hour prediction
  "num_coins": 10,
  "num_channels": 2,
  "target_coin": "XRP",
  "vocab_size": 256
}
```

### TrainedModelArtifact
```python
{
  "model_path": str,
  "history_path": str,
  "best_val_loss": float,
  "best_val_acc": float,
  "total_epochs": int
}
```

### EvalReportArtifact
```python
{
  "per_hour_accuracy": [float] * 8,
  "sequence_accuracy": float,
  "baseline_results": {
    "random": float,
    "persistence": float
  }
}
```

---

## Key Design Decisions

| Decision | Rationale |
|----------|-----------|
| 24-hour input | Captures daily cycles, trains faster than 48h |
| 2 channels | Price + volume captures strength & direction |
| 256-bin vocab | Fine-grained continuous quantization, interpretable |
| Quantile binning | Auto-balanced distribution, coin-adaptive |
| Fit on train only | Prevents data leakage |
| Decoder-only | Causal masking prevents future leakage |
| Teacher forcing | Stable training, fast convergence |
| 8-hour horizon | Practical for trading, measurable |

---

## Success Criteria

### Pipeline Validation
- ✅ Tokenization produces uniform distribution on train (each bin ~0.39%)
- ✅ No data leakage (val distribution may differ)
- ✅ All tensor shapes correct
- ✅ Token values in range [0, 255]

### Model Performance
- 🎯 Hour-1 accuracy > 25% (baseline: ~0.39% random per bin)
- 🎯 Hour-8 accuracy > 20%
- 🎯 Sequence accuracy > 0.01%
- 🎯 Beats all baselines significantly

### Deployment
- ✅ Inference < 100ms
- ✅ Model size < 500MB
- ✅ Reproducible results

---

## Known Limitations

1. **Accuracy challenge**: 256 classes is significantly harder than 3 classes
2. **Accuracy degrades over horizon**: 8-hour predictions inherently uncertain
3. **No uncertainty calibration**: Softmax probabilities ≠ true confidence
4. **Macro event blindness**: Model can't handle news/crashes outside training distribution
5. **Single-coin output**: Only predicts XRP
6. **Fixed 8-hour horizon**: Not adaptive to user preferences

---

## Computational Requirements

- **Training**: 1 GPU (RTX 3080+), ~3-6 hours for 100 epochs (larger vocab than 3-class), 3-5 GB GPU memory
- **Inference**: CPU sufficient, <50ms per prediction
- **Storage**: <200 MB total (data + model)

---

## Implementation Notes

1. **Anti-leakage**: Bin edges must be fit on train data only, then applied to val/inference
2. **Single-step output**: Model predicts only 1 token (next hour), not 8
3. **Teacher forcing**: During training, use ground truth next-hour token as target
4. **Autoregressive generation**: During eval/inference, loop 8 times:
   - Forward pass through decoder
   - Get logits for all 256 classes
   - Select token (argmax or sample)
   - Append predicted token to sequence
   - Remove first (oldest) hour to maintain 24-hour window
   - Repeat
5. **Causal masking**: Position i can only attend to positions ≤ i (prevents future leakage)
6. **Channel handling**: Separate embeddings for price/volume, then fuse
7. **Target extraction**: Only XRP price channel used for targets, not volume
8. **Temporal separation**: Input = hours [0-23], Target = hour [24] (no overlap)

---

## CLI Interface

```bash
# Full pipeline
python main.py pipeline run-all

# Individual steps
python main.py pipeline tokenize
python main.py pipeline sequences
python main.py pipeline train
python main.py pipeline evaluate
python main.py pipeline inference
```

---

## Testing Requirements

1. **Unit tests**: Each pipeline block, model forward/generate
2. **Integration tests**: End-to-end pipeline execution
3. **Shape validation**: All tensor dimensions correct
4. **Leakage detection**: Val thresholds ≠ train thresholds
5. **Reproducibility**: Same seed → same results
