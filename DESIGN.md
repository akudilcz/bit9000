# Simple Multi-Coin Token Predictor - Technical Specification

## Overview

**Objective**: Train a lightweight transformer decoder to predict the next hour's XRP price movement using 24 hours of multi-coin token sequences. Generate 8-hour forecasts autoregressively at inference time.

**Input**: `(24, 10, 2)` - 24 hours Ã— 10 coins Ã— 2 channels (price + volume)  
**Output**: `(1,)` - Next hour XRP price direction token  
**Vocabulary**: 256 bins `{0-255}` for continuous price quantization  
**Architecture**: Transformer decoder-only with causal masking, autoregressive generation

---

## Data Flow Summary

```
Step 0: Reset
  â””â”€> Clean artifacts directory

Step 1: Download
  â””â”€> OHLCV data (2020-2025, 10 coins, hourly)
      â”œâ”€ raw_data.parquet [T Ã— (10 coins Ã— 5 OHLCV)]
      â””â”€ Visualizations: price trends, data quality

Step 2: Clean
  â””â”€> Fill gaps, remove outliers
      â”œâ”€ clean_data.parquet [T Ã— (10 coins Ã— 5 OHLCV)]
      â””â”€ Quality metrics: % filled, % outliers

Step 3: Split
  â””â”€> Temporal split (80% train, 20% val)
      â”œâ”€ train_clean.parquet [T_train Ã— (10 coins Ã— 5)]
      â””â”€ val_clean.parquet [T_val Ã— (10 coins Ã— 5)]

Step 4: Tokenize
  â””â”€> Convert price/volume to 256-bin tokens
      â”œâ”€ Compute log returns (price) and log changes (volume)
      â”œâ”€ Fit bin edges (0-255) based on percentiles on train data
      â”œâ”€ Apply bins to train & val
      â”œâ”€ train_tokens.parquet [T_train Ã— 20 cols: COIN_price, COIN_volume]
      â”œâ”€ val_tokens.parquet [T_val Ã— 20 cols]
      â””â”€ fitted_thresholds.json {coin: {price: [bin_edges], volume: [bin_edges]}}

Step 5: Sequences
  â””â”€> Create rolling windows (24h input â†’ 1h target)
      â”œâ”€ train_X.pt [(N_train, 24, 10, 2)] - all coins, 2 channels
      â”œâ”€ train_y.pt [(N_train,)] - next hour XRP price token only
      â”œâ”€ val_X.pt [(N_val, 24, 10, 2)]
      â””â”€ val_y.pt [(N_val,)]

Step 6: Train
  â””â”€> Transformer decoder with teacher forcing
      â”œâ”€ Embed price & volume tokens separately (256 vocab)
      â”œâ”€ Fuse channels â†’ d_model=256
      â”œâ”€ Aggregate coins (mean pooling)
      â”œâ”€ Apply positional encoding
      â”œâ”€ Transformer decoder (4 layers, causal mask)
      â”œâ”€ Output head â†’ 256 classes (next 1 hour)
      â”œâ”€ model.pt (best checkpoint)
      â””â”€ history.json (loss/accuracy curves)

Step 7: Evaluate
  â””â”€> Autoregressive generation on validation set
      â”œâ”€ Generate 8 steps ahead autoregressively
      â”œâ”€ Per-hour accuracy (hours 1-8)
      â”œâ”€ Sequence accuracy (all 8 correct)
      â”œâ”€ Baseline comparison (persistence, random)
      â””â”€ eval_results.json + confusion matrices

Step 8: Inference
  â””â”€> Real-time prediction
      â”œâ”€ Fetch last 24h of OHLCV
      â”œâ”€ Tokenize with fitted bin edges
      â”œâ”€ Autoregressive generation: loop 8 times predicting 1 step at a time
      â”œâ”€ Each step: append predicted token to input, shift window, predict next
      â””â”€ predictions.json {hour: 1-8, token, probabilities}
```

**Key Transformations**:
- OHLCV â†’ Tokens: `log(price[t]/price[t-1])` â†’ quantize to 256 bins â†’ `{0-255}`
- Tokens â†’ Sequences: sliding window (stride=1) â†’ `(24h, 10 coins, 2 ch)` inputs, 1 target
- Training: Teacher forcing with ground truth next-hour target
- Inference: Autoregressive generation - predict 1 step, append to sequence, predict next

---

## Design Principles

1. **Single-step prediction**: Model predicts only the next hour; simplicity and stability
2. **Autoregressive generation**: 8-hour forecasts generated by iterative 1-step predictions
3. **Continuous quantization**: 256 bins provide fine-grained price movement representation
4. **Uniform binning**: Bins fit on training data percentiles for balanced distribution
5. **Multi-coin context**: BTC/ETH patterns inform XRP predictions
6. **No data leakage**: Fit bin edges on training data only
7. **Decoder-only**: Causal masking prevents future information leakage

---

## Pipeline Steps

### Step 0: Reset

**Purpose**: Clear all previous pipeline artifacts and start fresh

**Process**:
1. Remove all contents from `artifacts/` directory
2. Initialize empty directory structure for each pipeline step
3. Create reset metadata artifact

**Artifacts**:
- `reset_artifact.json`: Metadata about reset operation (timestamp, path cleared)

---

### Step 1: Download

**Purpose**: Fetch historical OHLCV (Open, High, Low, Close, Volume) data for 10 cryptocurrencies

**Input**: None (external data source: CoinGecko, Binance, or similar API)

**Output**: Raw OHLCV data in parquet format

**Process**:
1. Query data API for 10 coins: BTC, ETH, BNB, XRP, SOL, DOGE, ADA, AVAX, DOT, LTC
2. Time period: 2020-01-01 to 2025-12-31 (6 years of hourly data)
3. For each coin, fetch: Open, High, Low, Close, Volume
4. Handle missing data gracefully (record gaps)
5. Validate data quality: check for NaNs, zero volumes, price ranges
6. Save as parquet for efficient storage and loading

**Artifacts**:
- `raw_data.parquet`: Shape `[T, 50]` where T is number of hourly timestamps
  - Columns: BTC_open, BTC_high, BTC_low, BTC_close, BTC_volume, ETH_open, ... (5 cols Ã— 10 coins)
- `download_artifact.json`: Metadata (coins, date range, rows downloaded, missing data %)
- Visualizations: Price trends over time for each coin, data completeness heatmap

---

### Step 2: Clean

**Purpose**: Fill gaps, handle outliers, and ensure data quality

**Input**: Raw OHLCV data from Step 1

**Output**: Cleaned OHLCV data ready for analysis

**Process**:
1. **Fill gaps**: Forward fill for short gaps (<24 hours), remove coins/periods with long gaps
2. **Outlier detection**: Flag extreme price movements (e.g., >50% in 1 hour) - investigate or interpolate
3. **Volume validation**: Remove zero-volume periods, flag suspicious patterns
4. **Price validation**: Ensure High â‰¥ Close â‰¥ Low â‰¥ 0
5. **Alignment**: Ensure all coins have same timestamps (reindex if needed)
6. **Normalization**: All prices in USD, volumes in base currency units

**Artifacts**:
- `clean_data.parquet`: Same shape as raw data, but cleaned and aligned
  - Shape: `[T', 50]` where T' â‰¤ T (fewer rows if long gaps removed)
- `clean_artifact.json`: Metadata
  - Gaps filled: % and number
  - Outliers detected/fixed: count per coin
  - Time range after cleaning
  - Quality score: 0-100

---

### Step 3: Split

**Purpose**: Temporal train/validation split to prevent data leakage

**Input**: Clean OHLCV data from Step 2

**Output**: Train and validation sets separated by time

**Process**:
1. Calculate split point: 80% of timestamps for training, 20% for validation
2. Temporal split (NOT random): maintain temporal order
   - Example: Train = 2020-2024, Val = 2024-2025
3. Split each of the 10 coins independently (each coin gets same split point)
4. Save as separate parquet files

**Rationale for temporal split**:
- Prevents **data leakage**: Model can't see future data during training
- Reflects real-world deployment: model makes predictions on future unseen data
- Avoids look-ahead bias: validation only contains data after training data

**Artifacts**:
- `train_clean.parquet`: Training set
  - Shape: `[T_train, 50]` where T_train â‰ˆ 0.8 Ã— T'
- `val_clean.parquet`: Validation set
  - Shape: `[T_val, 50]` where T_val â‰ˆ 0.2 Ã— T'
- `split_artifact.json`: Metadata
  - Train rows: T_train, date range
  - Val rows: T_val, date range
  - Split ratio: 0.8 / 0.2
  - No temporal overlap confirmed

### Step 4: Tokenization

**Input**: Clean OHLCV data  
**Output**: Token DataFrames with 2 channels per coin

**Process**:
1. **Fit Phase** (training data only):
   - Compute log returns: `r_price = log(close[t] / close[t-1])`
   - Compute log changes: `r_volume = log(volume[t] / volume[t-1])`
   - Calculate 256 bin edges (0-255) using quantile-based binning per coin per channel
   - Save bin edges: `{coin: {price: [edge_0, edge_1, ..., edge_255], volume: [...]}`

2. **Transform Phase** (train + val):
   - Apply fitted bins using `np.digitize()`:
     - Token = bin_index based on which quantile interval the return falls into
     - Range: 0-255 for each price/volume return

**Artifacts**:
- `train_tokens.parquet`: columns like `BTC_price`, `BTC_volume`, etc. (values 0-255)
- `val_tokens.parquet`
- `fitted_thresholds.json`
- Visualizations: token distribution, bin edge heatmap

### Step 5: Sequence Creation

**Input**: Tokenized DataFrames  
**Output**: PyTorch tensors

**Process**:
- Create rolling windows (stride=1):
  - `X`: 24 consecutive hours Ã— all coins Ã— 2 channels
  - `y`: next single hour of XRP price token only
- Drop incomplete windows

**Tensor Shapes**:
- `train_X.pt`: `(N_train, 24, 10, 2)` dtype=long
- `train_y.pt`: `(N_train,)` dtype=long (single next-hour token)
- `val_X.pt`: `(N_val, 24, 10, 2)` dtype=long
- `val_y.pt`: `(N_val,)` dtype=long (single next-hour token)

### Step 6: Model Training

**Architecture**: Transformer Decoder-Only

**Components**:
1. **Token Embeddings**: Separate embeddings for price (256Ã—64) and volume (256Ã—64)
2. **Channel Fusion**: Concatenate and project to `d_model=256`
3. **Coin Aggregation**: Mean pooling across coins per timestep
4. **Positional Encoding**: Sinusoidal (max_len = 24)
5. **Transformer Decoder**: 4 layers, 4 heads, causal masking
6. **Output Head**: Linear projection to 256 classes (predicts next hour only)

**Training**:
- **Loss**: Cross-entropy with teacher forcing
- **Optimizer**: AdamW (lr=1e-4, weight_decay=1e-5)
- **Batch size**: 256
- **Gradient clipping**: max_norm=1.0
- **Scheduler**: ReduceLROnPlateau (factor=0.5, patience=10, min_lr=1e-6)
- **Early stopping**: patience=20 epochs
- **Warmup**: 5 epochs (1e-7 â†’ 1e-4)

**Artifacts**:
- `model.pt`: Best checkpoint (by val loss)
- `history.json`: Training metrics
- Visualizations: loss curves, accuracy curves

### Step 7: Evaluation

**Process**:
1. For each validation sample, perform autoregressive generation:
   - Start with 24-hour input window
   - Predict next hour (hour 1)
   - Append prediction to sequence and remove first hour (shift window)
   - Repeat 7 more times to get 8 predictions

**Metrics**:
1. **Per-hour accuracy**: Accuracy at each of 8 prediction steps (hour 1 is most accurate, hour 8 least)
2. **Sequence accuracy**: % where all 8 predictions correct
3. **Confusion matrices**: Per-hour (hours 1, 4, 8)
4. **Baseline comparisons**:
   - Persistence: repeat last token for all 8 hours
   - Random: uniform distribution for each hour

**Target Performance**:
- Hour-1 accuracy > 25% (baseline: ~0.39% random per bin)
- Hour-8 accuracy > 20%
- Sequence accuracy > 0.01%

**Artifacts**:
- `eval_results.json`: All metrics
- Visualizations: accuracy decay, confusion matrices, baseline comparison

### Step 8: Inference

**Process**:
1. Fetch last 24+1 hours of OHLCV data
2. Tokenize using fitted bin edges (2 channels)
3. Create initial tensor: `(1, 24, 10, 2)` (batch_size=1)
4. Loop 8 times:
   - Run model: get logits for next hour (256 classes)
   - Sample/argmax to get predicted token
   - Append prediction to sequence
   - Remove first hour from sequence (keep it 24 hours long)
5. Return 8 predictions with probabilities

**Output Format**:
```json
{
  "timestamp": "2025-10-23T12:00:00Z",
  "coin": "XRP",
  "horizon_hours": 8,
  "predictions": [
    {
      "hour": 1,
      "prediction": 128,
      "confidence": 0.57,
      "probabilities": [0.001, 0.002, ..., 0.57, ...]  # 256 values
    },
    ...
  ]
}
```

---

## Model Specification

### SimpleTokenPredictor

**Parameters**:
- `vocab_size`: 256
- `embedding_dim`: 64
- `d_model`: 256 (embedding_dim Ã— 2 after channel fusion)
- `num_heads`: 4
- `num_layers`: 4
- `feedforward_dim`: 512
- `dropout`: 0.1
- `input_length`: 24
- `num_coins`: 10
- `num_classes`: 256
- `num_channels`: 2

**Methods**:
- `forward(x, targets=None)`: Training with teacher forcing, outputs logits (batch_size, 256)
- `generate(x, steps=8)`: Autoregressive inference, generates `steps` tokens sequentially

**Model Size**: ~8.5M parameters â‰ˆ 34 MB (fp32) - larger due to 256 vocab

---

## Configuration (config.yaml)

```yaml
data:
  coins: [BTC, ETH, BNB, XRP, SOL, DOGE, ADA, AVAX, DOT, LTC]
  target_coin: XRP
  interval: 1h

split:
  train_ratio: 0.8
  temporal: true

tokenization:
  vocab_size: 256
  method: quantile
  percentiles: [1, 2, 3, ..., 99]  # 255 bin edges for 256 bins

sequences:
  input_length: 24
  output_length: 1  # Single next-hour prediction
  num_channels: 2

model:
  type: SimpleTokenPredictor
  vocab_size: 256
  num_classes: 256
  num_coins: 10
  embedding_dim: 64
  d_model: 256
  num_heads: 4
  num_layers: 4
  feedforward_dim: 512
  dropout: 0.1

training:
  device: cuda
  epochs: 100
  batch_size: 256
  learning_rate: 0.0001
  weight_decay: 0.00001
  label_smoothing: 0.0
  max_grad_norm: 1.0
  early_stopping:
    patience: 20
  warmup:
    epochs: 5
    start_lr: 0.0000001
  scheduler:
    type: ReduceLROnPlateau
    factor: 0.5
    patience: 10
    min_lr: 0.000001

inference:
  num_steps: 8  # Generate 8 steps autoregressively
  sampling: argmax  # or 'sample' for stochastic generation
```

---

## Artifact Structure

```
artifacts/
â”œâ”€â”€ step_04_tokenize/
â”‚   â”œâ”€â”€ train_tokens.parquet
â”‚   â”œâ”€â”€ val_tokens.parquet
â”‚   â”œâ”€â”€ fitted_thresholds.json
â”‚   â””â”€â”€ tokenize_artifact.json
â”œâ”€â”€ step_05_sequences/
â”‚   â”œâ”€â”€ train_X.pt  # (N, 24, 10, 2)
â”‚   â”œâ”€â”€ train_y.pt  # (N, 8)
â”‚   â”œâ”€â”€ val_X.pt
â”‚   â”œâ”€â”€ val_y.pt
â”‚   â””â”€â”€ sequences_artifact.json
â”œâ”€â”€ step_06_train/
â”‚   â”œâ”€â”€ model.pt
â”‚   â”œâ”€â”€ history.json
â”‚   â””â”€â”€ train_artifact.json
â”œâ”€â”€ step_07_evaluate/
â”‚   â”œâ”€â”€ eval_results.json
â”‚   â””â”€â”€ evaluate_artifact.json
â””â”€â”€ step_08_inference/
    â”œâ”€â”€ predictions_{timestamp}.json
    â””â”€â”€ inference_artifact.json
```

---

## Data Schemas

### TokenizeArtifact
```python
{
  "train_path": str,
  "val_path": str,
  "train_shape": tuple,
  "val_shape": tuple,
  "thresholds_path": str,
  "token_distribution": {
    bin_id: {"train": float, "val": float}
    for bin_id in range(256)
  }
}
```

### SequencesArtifact
```python
{
  "train_X_path": str,
  "train_y_path": str,
  "val_X_path": str,
  "val_y_path": str,
  "train_num_samples": int,
  "val_num_samples": int,
  "input_length": 24,
  "output_length": 1,  # Single next-hour prediction
  "num_coins": 10,
  "num_channels": 2,
  "target_coin": "XRP",
  "vocab_size": 256
}
```

### TrainedModelArtifact
```python
{
  "model_path": str,
  "history_path": str,
  "best_val_loss": float,
  "best_val_acc": float,
  "total_epochs": int
}
```

### EvalReportArtifact
```python
{
  "per_hour_accuracy": [float] * 8,
  "sequence_accuracy": float,
  "baseline_results": {
    "random": float,
    "persistence": float
  }
}
```

---

## Key Design Decisions

| Decision | Rationale |
|----------|-----------|
| 24-hour input | Captures daily cycles, trains faster than 48h |
| 2 channels | Price + volume captures strength & direction |
| 256-bin vocab | Fine-grained continuous quantization, interpretable |
| Quantile binning | Auto-balanced distribution, coin-adaptive |
| Fit on train only | Prevents data leakage |
| Decoder-only | Causal masking prevents future leakage |
| Teacher forcing | Stable training, fast convergence |
| 8-hour horizon | Practical for trading, measurable |

---

## Success Criteria

### Pipeline Validation
- âœ… Tokenization produces uniform distribution on train (each bin ~0.39%)
- âœ… No data leakage (val distribution may differ)
- âœ… All tensor shapes correct
- âœ… Token values in range [0, 255]

### Model Performance
- ðŸŽ¯ Hour-1 accuracy > 25% (baseline: ~0.39% random per bin)
- ðŸŽ¯ Hour-8 accuracy > 20%
- ðŸŽ¯ Sequence accuracy > 0.01%
- ðŸŽ¯ Beats all baselines significantly

### Deployment
- âœ… Inference < 100ms
- âœ… Model size < 500MB
- âœ… Reproducible results

---

## Known Limitations

1. **Accuracy challenge**: 256 classes is significantly harder than 3 classes
2. **Accuracy degrades over horizon**: 8-hour predictions inherently uncertain
3. **No uncertainty calibration**: Softmax probabilities â‰  true confidence
4. **Macro event blindness**: Model can't handle news/crashes outside training distribution
5. **Single-coin output**: Only predicts XRP
6. **Fixed 8-hour horizon**: Not adaptive to user preferences

---

## Computational Requirements

- **Training**: 1 GPU (RTX 3080+), ~3-6 hours for 100 epochs (larger vocab than 3-class), 3-5 GB GPU memory
- **Inference**: CPU sufficient, <50ms per prediction
- **Storage**: <200 MB total (data + model)

---

## Implementation Notes

1. **Anti-leakage**: Bin edges must be fit on train data only, then applied to val/inference
2. **Single-step output**: Model predicts only 1 token (next hour), not 8
3. **Teacher forcing**: During training, use ground truth next-hour token as target
4. **Autoregressive generation**: During eval/inference, loop 8 times:
   - Forward pass through decoder
   - Get logits for all 256 classes
   - Select token (argmax or sample)
   - Append predicted token to sequence
   - Remove first (oldest) hour to maintain 24-hour window
   - Repeat
5. **Causal masking**: Position i can only attend to positions â‰¤ i (prevents future leakage)
6. **Channel handling**: Separate embeddings for price/volume, then fuse
7. **Target extraction**: Only XRP price channel used for targets, not volume
8. **Temporal separation**: Input = hours [0-23], Target = hour [24] (no overlap)

---

## CLI Interface

```bash
# Full pipeline
python main.py pipeline run-all

# Individual steps
python main.py pipeline tokenize
python main.py pipeline sequences
python main.py pipeline train
python main.py pipeline evaluate
python main.py pipeline inference
```

---

## Testing Requirements

1. **Unit tests**: Each pipeline block, model forward/generate
2. **Integration tests**: End-to-end pipeline execution
3. **Shape validation**: All tensor dimensions correct
4. **Leakage detection**: Val thresholds â‰  train thresholds
5. **Reproducibility**: Same seed â†’ same results
