
[TUNE] HYPERPARAMETER TUNING: Optimizing model hyperparameters...
Missing 20 required config value(s):
  - binning.lookback_hours
  - binning.target_shift_hours
  - binning.num_bins
  - binning.method
  - binning.quartiles
  - binning.labels
  - binning.feature_engineering_params
  - binning.feature_engineering_params.epsilon
  - binning.feature_engineering_params.max_ratio
  - binning.feature_engineering_params.max_volume_ratio
  - binning.feature_engineering_params.rsi_period
  - binning.feature_engineering_params.rsi_min_periods
  - binning.feature_engineering_params.rsi_default
  - binning.feature_engineering_params.vol_window
  - binning.feature_engineering_params.vol_min_periods
  - binning.feature_engineering_params.momentum_4h
  - binning.feature_engineering_params.momentum_24h
  - model.num_target_coins
  - model.features_per_coin
  - model.sequence_length

Please ensure all required config values are defined in config.yaml
Loading sequences from artifacts\step_05_sequences...
  Train: X=torch.Size([39455, 24, 10, 2]), y=torch.Size([39455, 1])
  Val:   X=torch.Size([4295, 24, 10, 2]), y=torch.Size([4295, 1])

Starting tuning with 20 trials, 25 epochs per trial...
2025-10-24 16:30:50,541 - src.pipeline.io - INFO - Loaded train: X=torch.Size([39455, 24, 10, 2]), y=torch.Size([39455, 1])
2025-10-24 16:30:50,541 - src.pipeline.io - INFO - Loaded val: X=torch.Size([4295, 24, 10, 2]), y=torch.Size([4295, 1])
2025-10-24 16:30:50,552 - src.pipeline.io - INFO - Using device: cuda
2025-10-24 16:30:50,552 - src.pipeline.io - INFO - Starting hyperparameter tuning with 20 trials
2025-10-24 16:30:50,552 - src.pipeline.io - INFO - Each trial will train for 25 epochs
[I 2025-10-24 16:30:50,553] A new study created in memory with name: no-name-c8216263-715c-4146-966a-ff5b3978bad7

  0%|          | 0/20 [00:00<?, ?it/s]2025-10-24 16:30:50,555 - src.pipeline.io - INFO - Trial 0: Testing params {'learning_rate': 6.64645947158108e-05, 'weight_decay': 0.005669849511478858, 'dropout': 0.36599697090570255, 'label_smoothing': 0.05986584841970366, 'max_grad_norm': 0.7340279606636548, 'batch_size': 128, 'embedding_dim': 16, 'd_model': 64, 'num_layers': 2, 'num_heads': 8, 'feedforward_dim': 1024, 'warmup_epochs': 1}
2025-10-24 16:30:50,564 - src.pipeline.io - INFO - Initialized SimpleTokenPredictor with 362,752 parameters
2025-10-24 16:30:50,564 - src.pipeline.io - INFO -   Input: 24 hours Î 10 coins Î 2 channels
2025-10-24 16:30:50,564 - src.pipeline.io - INFO -   Output: 1 hour (next hour prediction, autoregressively generated to 8 hours)
2025-10-24 16:30:50,564 - src.pipeline.io - INFO -   Vocab: 256 bins (0-255 for continuous quantization)
2025-10-24 16:30:50,564 - src.pipeline.io - INFO -   Model dim: 64, Heads: 8, Layers: 2
2025-10-24 16:30:53,632 - src.pipeline.io - INFO - Trial 0 Epoch 1/25: train_loss=2.8808, val_loss=1.7726
2025-10-24 16:30:55,787 - src.pipeline.io - INFO - Trial 0 Epoch 2/25: train_loss=1.6995, val_loss=1.5948
2025-10-24 16:30:57,973 - src.pipeline.io - INFO - Trial 0 Epoch 3/25: train_loss=1.6009, val_loss=1.5861
2025-10-24 16:31:00,176 - src.pipeline.io - INFO - Trial 0 Epoch 4/25: train_loss=1.5802, val_loss=1.5802
2025-10-24 16:31:02,345 - src.pipeline.io - INFO - Trial 0 Epoch 5/25: train_loss=1.5674, val_loss=1.5805
2025-10-24 16:31:04,523 - src.pipeline.io - INFO - Trial 0 Epoch 6/25: train_loss=1.5667, val_loss=1.5814
2025-10-24 16:31:06,751 - src.pipeline.io - INFO - Trial 0 Epoch 7/25: train_loss=1.5632, val_loss=1.5803
2025-10-24 16:31:09,470 - src.pipeline.io - INFO - Trial 0 Epoch 8/25: train_loss=1.5629, val_loss=1.5798
2025-10-24 16:31:12,320 - src.pipeline.io - INFO - Trial 0 Epoch 9/25: train_loss=1.5605, val_loss=1.5797
2025-10-24 16:31:15,032 - src.pipeline.io - INFO - Trial 0 Epoch 10/25: train_loss=1.5613, val_loss=1.5801
2025-10-24 16:31:17,657 - src.pipeline.io - INFO - Trial 0 Epoch 11/25: train_loss=1.5605, val_loss=1.5793
2025-10-24 16:31:20,401 - src.pipeline.io - INFO - Trial 0 Epoch 12/25: train_loss=1.5608, val_loss=1.5791
2025-10-24 16:31:22,673 - src.pipeline.io - INFO - Trial 0 Epoch 13/25: train_loss=1.5595, val_loss=1.5791

[TUNE] HYPERPARAMETER TUNING: Optimizing model hyperparameters...
Missing 20 required config value(s):
  - binning.lookback_hours
  - binning.target_shift_hours
  - binning.num_bins
  - binning.method
  - binning.quartiles
  - binning.labels
  - binning.feature_engineering_params
  - binning.feature_engineering_params.epsilon
  - binning.feature_engineering_params.max_ratio
  - binning.feature_engineering_params.max_volume_ratio
  - binning.feature_engineering_params.rsi_period
  - binning.feature_engineering_params.rsi_min_periods
  - binning.feature_engineering_params.rsi_default
  - binning.feature_engineering_params.vol_window
  - binning.feature_engineering_params.vol_min_periods
  - binning.feature_engineering_params.momentum_4h
  - binning.feature_engineering_params.momentum_24h
  - model.num_target_coins
  - model.features_per_coin
  - model.sequence_length

Please ensure all required config values are defined in config.yaml
Loading sequences from artifacts\step_05_sequences...
  Train: X=torch.Size([39455, 24, 10, 2]), y=torch.Size([39455, 1])
  Val:   X=torch.Size([4295, 24, 10, 2]), y=torch.Size([4295, 1])

Starting tuning with 20 trials, 25 epochs per trial...
2025-10-24 16:31:55,252 - src.pipeline.io - INFO - Loaded train: X=torch.Size([39455, 24, 10, 2]), y=torch.Size([39455, 1])
2025-10-24 16:31:55,252 - src.pipeline.io - INFO - Loaded val: X=torch.Size([4295, 24, 10, 2]), y=torch.Size([4295, 1])
2025-10-24 16:31:55,264 - src.pipeline.io - INFO - Using device: cuda
2025-10-24 16:31:55,264 - src.pipeline.io - INFO - Starting hyperparameter tuning with 20 trials
2025-10-24 16:31:55,264 - src.pipeline.io - INFO - Each trial will train for 25 epochs
[I 2025-10-24 16:31:55,264] A new study created in memory with name: no-name-6e2e7e05-f132-43d6-b937-a7a8571a90a1

  0%|          | 0/20 [00:00<?, ?it/s]2025-10-24 16:31:55,266 - src.pipeline.io - INFO - Trial 0: Testing params {'learning_rate': 6.64645947158108e-05, 'weight_decay': 0.005669849511478858, 'dropout': 0.36599697090570255, 'label_smoothing': 0.05986584841970366, 'max_grad_norm': 0.7340279606636548, 'batch_size': 128, 'embedding_dim': 16, 'd_model': 64, 'num_layers': 2, 'num_heads': 8, 'feedforward_dim': 1024, 'warmup_epochs': 1}
2025-10-24 16:31:55,274 - src.pipeline.io - INFO - Initialized SimpleTokenPredictor with 362,752 parameters
2025-10-24 16:31:55,276 - src.pipeline.io - INFO -   Input: 24 hours Î 10 coins Î 2 channels
2025-10-24 16:31:55,276 - src.pipeline.io - INFO -   Output: 1 hour (next hour prediction, autoregressively generated to 8 hours)
2025-10-24 16:31:55,276 - src.pipeline.io - INFO -   Vocab: 256 bins (0-255 for continuous quantization)
2025-10-24 16:31:55,276 - src.pipeline.io - INFO -   Model dim: 64, Heads: 8, Layers: 2
2025-10-24 16:31:58,449 - src.pipeline.io - INFO - Trial 0 Epoch 1/25: train_loss=2.8421, val_loss=1.7509
2025-10-24 16:32:00,562 - src.pipeline.io - INFO - Trial 0 Epoch 2/25: train_loss=1.7055, val_loss=1.5883
2025-10-24 16:32:02,707 - src.pipeline.io - INFO - Trial 0 Epoch 3/25: train_loss=1.6032, val_loss=1.5876
2025-10-24 16:32:04,989 - src.pipeline.io - INFO - Trial 0 Epoch 4/25: train_loss=1.5863, val_loss=1.5832
2025-10-24 16:32:07,213 - src.pipeline.io - INFO - Trial 0 Epoch 5/25: train_loss=1.5680, val_loss=1.5829
2025-10-24 16:32:09,464 - src.pipeline.io - INFO - Trial 0 Epoch 6/25: train_loss=1.5667, val_loss=1.5836
2025-10-24 16:32:11,662 - src.pipeline.io - INFO - Trial 0 Epoch 7/25: train_loss=1.5639, val_loss=1.5849
2025-10-24 16:32:13,835 - src.pipeline.io - INFO - Trial 0 Epoch 8/25: train_loss=1.5632, val_loss=1.5821
2025-10-24 16:32:16,069 - src.pipeline.io - INFO - Trial 0 Epoch 9/25: train_loss=1.5634, val_loss=1.5819
2025-10-24 16:32:18,309 - src.pipeline.io - INFO - Trial 0 Epoch 10/25: train_loss=1.5621, val_loss=1.5829
2025-10-24 16:32:20,492 - src.pipeline.io - INFO - Trial 0 Epoch 11/25: train_loss=1.5617, val_loss=1.5812
2025-10-24 16:32:22,724 - src.pipeline.io - INFO - Trial 0 Epoch 12/25: train_loss=1.5601, val_loss=1.5912
2025-10-24 16:32:24,942 - src.pipeline.io - INFO - Trial 0 Epoch 13/25: train_loss=1.5595, val_loss=1.5809
2025-10-24 16:32:27,003 - src.pipeline.io - INFO - Trial 0 Epoch 14/25: train_loss=1.5605, val_loss=1.5814
2025-10-24 16:32:29,171 - src.pipeline.io - INFO - Trial 0 Epoch 15/25: train_loss=1.5598, val_loss=1.5797
2025-10-24 16:32:31,260 - src.pipeline.io - INFO - Trial 0 Epoch 16/25: train_loss=1.5591, val_loss=1.5799
2025-10-24 16:32:33,413 - src.pipeline.io - INFO - Trial 0 Epoch 17/25: train_loss=1.5580, val_loss=1.5793
2025-10-24 16:32:35,508 - src.pipeline.io - INFO - Trial 0 Epoch 18/25: train_loss=1.5583, val_loss=1.5793
2025-10-24 16:32:37,602 - src.pipeline.io - INFO - Trial 0 Epoch 19/25: train_loss=1.5582, val_loss=1.5798
2025-10-24 16:32:39,709 - src.pipeline.io - INFO - Trial 0 Epoch 20/25: train_loss=1.5571, val_loss=1.5795
2025-10-24 16:32:41,784 - src.pipeline.io - INFO - Trial 0 Epoch 21/25: train_loss=1.5564, val_loss=1.5799
2025-10-24 16:32:43,890 - src.pipeline.io - INFO - Trial 0 Epoch 22/25: train_loss=1.5562, val_loss=1.5787
2025-10-24 16:32:45,986 - src.pipeline.io - INFO - Trial 0 Epoch 23/25: train_loss=1.5559, val_loss=1.5797
2025-10-24 16:32:48,066 - src.pipeline.io - INFO - Trial 0 Epoch 24/25: train_loss=1.5562, val_loss=1.5785
2025-10-24 16:32:50,107 - src.pipeline.io - INFO - Trial 0 Epoch 25/25: train_loss=1.5554, val_loss=1.5788
2025-10-24 16:32:50,107 - src.pipeline.io - INFO - Trial 0 completed with best_val_loss=1.5785

                                      

  0%|          | 0/20 [00:54<?, ?it/s]
Best trial: 0. Best value: 1.57851:   0%|          | 0/20 [00:54<?, ?it/s]
Best trial: 0. Best value: 1.57851:   5%|5         | 1/20 [00:54<17:21, 54.84s/it]2025-10-24 16:32:50,109 - src.pipeline.io - INFO - Trial 1: Testing params {'learning_rate': 0.00017445094227456915, 'weight_decay': 9.163741808778786e-05, 'dropout': 0.023225206359998862, 'label_smoothing': 0.06075448519014384, 'max_grad_norm': 0.7557861855309373, 'batch_size': 128, 'embedding_dim': 16, 'd_model': 64, 'num_layers': 6, 'num_heads': 2, 'feedforward_dim': 256, 'warmup_epochs': 5}
2025-10-24 16:32:50,114 - src.pipeline.io - INFO - Initialized SimpleTokenPredictor with 431,616 parameters
2025-10-24 16:32:50,114 - src.pipeline.io - INFO -   Input: 24 hours Î 10 coins Î 2 channels
2025-10-24 16:32:50,114 - src.pipeline.io - INFO -   Output: 1 hour (next hour prediction, autoregressively generated to 8 hours)
2025-10-24 16:32:50,114 - src.pipeline.io - INFO -   Vocab: 256 bins (0-255 for continuous quantization)
2025-10-24 16:32:50,114 - src.pipeline.io - INFO -   Model dim: 64, Heads: 2, Layers: 6
2025-10-24 16:32:54,348 - src.pipeline.io - INFO - Trial 1 Epoch 1/25: train_loss=1.9589, val_loss=1.5881
2025-10-24 16:32:58,618 - src.pipeline.io - INFO - Trial 1 Epoch 2/25: train_loss=1.5642, val_loss=1.5762
2025-10-24 16:33:02,893 - src.pipeline.io - INFO - Trial 1 Epoch 3/25: train_loss=1.5615, val_loss=1.5751
2025-10-24 16:33:07,245 - src.pipeline.io - INFO - Trial 1 Epoch 4/25: train_loss=1.5598, val_loss=1.5804
2025-10-24 16:33:11,455 - src.pipeline.io - INFO - Trial 1 Epoch 5/25: train_loss=1.5587, val_loss=1.5758
2025-10-24 16:33:15,760 - src.pipeline.io - INFO - Trial 1 Epoch 6/25: train_loss=1.5575, val_loss=1.5791
2025-10-24 16:33:20,259 - src.pipeline.io - INFO - Trial 1 Epoch 7/25: train_loss=1.5565, val_loss=1.5800
2025-10-24 16:33:24,752 - src.pipeline.io - INFO - Trial 1 Epoch 8/25: train_loss=1.5583, val_loss=1.5744
2025-10-24 16:33:29,286 - src.pipeline.io - INFO - Trial 1 Epoch 9/25: train_loss=1.5562, val_loss=1.5738
2025-10-24 16:33:33,503 - src.pipeline.io - INFO - Trial 1 Epoch 10/25: train_loss=1.5566, val_loss=1.5757
2025-10-24 16:33:38,060 - src.pipeline.io - INFO - Trial 1 Epoch 11/25: train_loss=1.5560, val_loss=1.5753
2025-10-24 16:33:42,509 - src.pipeline.io - INFO - Trial 1 Epoch 12/25: train_loss=1.5558, val_loss=1.5752
2025-10-24 16:33:47,073 - src.pipeline.io - INFO - Trial 1 Epoch 13/25: train_loss=1.5556, val_loss=1.5751
2025-10-24 16:33:51,544 - src.pipeline.io - INFO - Trial 1 Epoch 14/25: train_loss=1.5555, val_loss=1.5740
